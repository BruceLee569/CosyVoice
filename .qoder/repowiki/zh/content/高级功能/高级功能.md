# 高级功能

<cite>
**本文档中引用的文件**   
- [cosyvoice.yaml](file://examples/libritts/cosyvoice/conf/cosyvoice.yaml)
- [cosyvoice2.yaml](file://examples/libritts/cosyvoice2/conf/cosyvoice2.yaml)
- [cosyvoice.py](file://cosyvoice/cli/cosyvoice.py)
- [train.py](file://cosyvoice/bin/train.py)
- [server.py](file://runtime/python/fastapi/server.py)
- [server.py](file://runtime/python/grpc/server.py)
- [docker-compose.yml](file://runtime/triton_trtllm/docker-compose.yml)
- [config.pbtxt](file://runtime/triton_trtllm/model_repo/cosyvoice2/config.pbtxt)
- [config.pbtxt](file://runtime/triton_trtllm/model_repo/speaker_embedding/config.pbtxt)
- [convert_checkpoint.py](file://runtime/triton_trtllm/scripts/convert_checkpoint.py)
- [extract_embedding.py](file://tools/extract_embedding.py)
- [extract_speech_token.py](file://tools/extract_speech_token.py)
</cite>

## 目录
1. [自定义模型配置](#自定义模型配置)
2. [训练新模型](#训练新模型)
3. [Triton Inference Server集成](#triton-inference-server集成)
4. [部署优化](#部署优化)
5. [工具脚本使用](#工具脚本使用)
6. [自定义前后端开发](#自定义前后端开发)
7. [高级用例](#高级用例)

## 自定义模型配置

CosyVoice的模型配置通过YAML文件进行管理，主要配置文件位于`examples/libritts/cosyvoice/conf/`目录下。核心配置文件`cosyvoice.yaml`定义了模型的各个组件和参数。

配置文件采用模块化设计，通过`!new`和`!ref`等特殊语法来实例化和引用组件。例如，LLM（大语言模型）组件通过`!new:cosyvoice.llm.llm.TransformerLM`进行实例化，并引用了文本编码器、LLM编码器等子组件。

关键配置参数包括：
- **sample_rate**: 采样率，CosyVoice-300M为22050Hz
- **text_encoder_input_size**: 文本编码器输入大小，512
- **llm_input_size**: LLM输入大小，1024
- **llm_output_size**: LLM输出大小，1024
- **spk_embed_dim**: 说话人嵌入维度，192

对于CosyVoice2，配置文件`cosyvoice2.yaml`位于`examples/libritts/cosyvoice2/conf/`目录下，其主要区别包括：
- **sample_rate**: 采样率提升至24000Hz
- 使用`Qwen2LM`作为LLM组件
- 引入了流式推理相关的参数，如`chunk_size`（流式推理块大小）

**Section sources**
- [cosyvoice.yaml](file://examples/libritts/cosyvoice/conf/cosyvoice.yaml#L1-L257)
- [cosyvoice2.yaml](file://examples/libritts/cosyvoice2/conf/cosyvoice2.yaml#L1-L234)

## 训练新模型

CosyVoice提供了完整的训练脚本，位于`cosyvoice/bin/train.py`。该脚本支持多种训练模式，包括传统的监督训练和直接偏好优化（DPO）训练。

训练流程主要包含以下步骤：
1. **参数解析**: 使用`argparse`解析命令行参数，包括训练引擎（`torch_ddp`或`deepspeed`）、模型类型、配置文件、训练数据等
2. **配置加载**: 使用`hyperpyyaml`加载YAML配置文件，并根据命令行参数进行覆盖
3. **分布式初始化**: 调用`init_distributed`函数初始化分布式训练环境
4. **数据集初始化**: 调用`init_dataset_and_dataloader`函数创建训练和验证数据集及数据加载器
5. **模型初始化**: 根据配置文件实例化模型，并加载预训练权重（如果提供）
6. **优化器和调度器初始化**: 调用`init_optimizer_and_scheduler`函数创建优化器和学习率调度器
7. **训练循环**: 通过`Executor`类执行训练循环，每个epoch调用`train_one_epoc`方法

对于DPO训练，需要额外提供参考模型（`ref_model`）参数，并在命令行中指定`--dpo`标志。DPO训练使用`DPOLoss`类计算损失，通过比较模型输出和参考模型输出来优化模型。

**Section sources**
- [train.py](file://cosyvoice/bin/train.py#L1-L196)

## Triton Inference Server集成

CosyVoice提供了与NVIDIA Triton Inference Server的集成支持，位于`runtime/triton_trtllm/`目录下。该集成利用TensorRT-LLM来加速CosyVoice2的LLM推理，相比HuggingFace Transformers实现可获得4倍的加速。

集成的核心组件包括：
- **Docker镜像**: `Dockerfile.server`定义了包含Triton Server和TensorRT-LLM的Docker镜像
- **模型仓库**: `model_repo/`目录包含多个模型的配置文件，包括`cosyvoice2`、`speaker_embedding`、`audio_tokenizer`等
- **配置文件**: 每个模型都有对应的`config.pbtxt`配置文件，定义了模型的输入输出、批处理策略等
- **启动脚本**: `run.sh`脚本管理整个工作流程，包括模型下载、转换、Triton Server启动等

`config.pbtxt`文件使用Protocol Buffers格式定义模型配置。以`cosyvoice2/config.pbtxt`为例，其关键配置包括：
- **name**: 模型名称
- **backend**: 后端类型，为`python`
- **max_batch_size**: 最大批处理大小，使用变量`${triton_max_batch_size}`动态配置
- **dynamic_batching**: 动态批处理配置，包含最大队列延迟
- **model_transaction_policy**: 模型事务策略，`decoupled`字段控制是否使用解耦模式
- **parameters**: 模型参数，包括`llm_tokenizer_dir`和`model_dir`
- **input**: 输入定义，包括参考音频、参考文本、目标文本等
- **output**: 输出定义，为生成的波形
- **instance_group**: 实例组配置，定义CPU实例数量

**Section sources**
- [config.pbtxt](file://runtime/triton_trtllm/model_repo/cosyvoice2/config.pbtxt#L1-L73)
- [config.pbtxt](file://runtime/triton_trtllm/model_repo/speaker_embedding/config.pbtxt#L1-L48)

## 部署优化

CosyVoice提供了多种部署优化方案，包括使用Docker进行容器编排和使用TensorRT-LLM进行推理加速。

### Docker容器编排

`runtime/triton_trtllm/`目录下的`docker-compose.yml`文件定义了Triton Server的Docker编排配置。该配置文件定义了一个名为`tts`的服务，其关键配置包括：
- **image**: 使用`soar97/triton-cosyvoice:25.06`镜像
- **shm_size**: 共享内存大小为1GB
- **ports**: 映射Triton Server的8000、8001、8002端口
- **environment**: 环境变量，包括`PYTHONIOENCODING`和`MODEL_ID`
- **deploy.resources.reservations.devices**: GPU设备预留，指定使用GPU 0
- **command**: 启动命令，包括安装依赖、克隆仓库和运行`run.sh`脚本

### 推理加速

通过`convert_checkpoint.py`脚本可以将HuggingFace格式的检查点转换为TensorRT-LLM格式。该脚本支持多种量化选项，包括：
- **--use_weight_only**: 启用权重量化
- **--weight_only_precision**: 权重量化精度，支持`int8`和`int4`
- **--smoothquant**: 启用SmoothQuant量化，需要指定α参数
- **--int8_kv_cache**: 启用KV缓存的int8量化

转换后的模型可以显著提升推理性能，特别是在高并发场景下。

**Section sources**
- [docker-compose.yml](file://runtime/triton_trtllm/docker-compose.yml#L1-L20)
- [convert_checkpoint.py](file://runtime/triton_trtllm/scripts/convert_checkpoint.py#L1-L331)

## 工具脚本使用

CosyVoice提供了多个实用工具脚本，位于`tools/`目录下，用于支持模型训练和推理的各种任务。

### 提取说话人嵌入

`extract_embedding.py`脚本用于从音频文件中提取说话人嵌入。该脚本的主要功能包括：
- 读取`wav.scp`文件，获取音频文件路径
- 读取`utt2spk`文件，获取音频与说话人的对应关系
- 使用ONNX运行时加载预训练的嵌入提取模型
- 对每个音频文件提取FBank特征并进行均值归一化
- 调用ONNX模型计算嵌入向量
- 将结果保存为PyTorch格式的`.pt`文件

该脚本支持多线程并行处理，通过`--num_thread`参数指定线程数。

### 提取语音标记

`extract_speech_token.py`脚本用于从音频文件中提取语音标记。该脚本的主要功能包括：
- 读取`wav.scp`文件，获取音频文件路径
- 使用ONNX运行时加载预训练的语音标记提取模型
- 对每个音频文件提取梅尔频谱图
- 调用ONNX模型计算语音标记
- 将结果保存为PyTorch格式的`.pt`文件

该脚本同样支持多线程并行处理，并针对长音频（超过30秒）进行了特殊处理。

**Section sources**
- [extract_embedding.py](file://tools/extract_embedding.py#L1-L78)
- [extract_speech_token.py](file://tools/extract_speech_token.py#L1-L73)

## 自定义前后端开发

CosyVoice提供了灵活的前后端接口，支持开发者根据需求进行自定义开发。

### 前端开发

前端功能主要由`cosyvoice/cli/frontend.py`文件中的`CosyVoiceFrontEnd`类实现。该类提供了多种前端处理方法：
- `frontend_sft`: 用于SFT（监督微调）模式的前端处理
- `frontend_zero_shot`: 用于零样本语音克隆的前端处理
- `frontend_cross_lingual`: 用于跨语言语音合成的前端处理
- `frontend_instruct`: 用于指令控制语音合成的前端处理

开发者可以基于这些方法开发自定义的前端处理逻辑，例如添加新的文本归一化规则或支持新的音频特征提取方法。

### 后端开发

后端服务提供了多种部署方式，包括FastAPI和gRPC。

#### FastAPI服务

`runtime/python/fastapi/server.py`文件实现了基于FastAPI的HTTP服务。该服务提供了多个端点：
- `/inference_sft`: SFT模式推理
- `/inference_zero_shot`: 零样本语音克隆推理
- `/inference_cross_lingual`: 跨语言语音合成推理
- `/inference_instruct`: 指令控制语音合成推理

服务使用`StreamingResponse`实现流式音频输出，支持实时语音合成。

#### gRPC服务

`runtime/python/grpc/server.py`文件实现了基于gRPC的服务。该服务定义了`CosyVoiceServicer`类，处理不同类型的推理请求，并通过流式RPC返回音频数据。

开发者可以根据业务需求选择合适的后端框架，并基于现有代码进行扩展和定制。

**Section sources**
- [cosyvoice.py](file://cosyvoice/cli/cosyvoice.py#L1-L239)
- [server.py](file://runtime/python/fastapi/server.py#L1-L102)
- [server.py](file://runtime/python/grpc/server.py#L1-L97)

## 高级用例

### 构建生产级语音合成服务

基于CosyVoice的高级功能，可以构建一个完整的生产级语音合成服务。以下是推荐的架构：

1. **模型准备**: 下载预训练模型并使用`convert_checkpoint.py`转换为TensorRT-LLM格式
2. **服务部署**: 使用`docker-compose.yml`部署Triton Inference Server
3. **前端接口**: 基于FastAPI或gRPC开发RESTful API接口
4. **负载均衡**: 在多个GPU节点上部署服务实例，使用负载均衡器分发请求
5. **缓存机制**: 对常用说话人的嵌入向量进行缓存，减少重复计算
6. **监控系统**: 集成Prometheus和Grafana监控服务性能和资源使用情况

通过这种架构，可以实现高并发、低延迟的语音合成服务，满足生产环境的需求。