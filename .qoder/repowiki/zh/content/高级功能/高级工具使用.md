# 高级工具使用

<cite>
**本文档中引用的文件**  
- [extract_embedding.py](file://tools/extract_embedding.py)
- [extract_speech_token.py](file://tools/extract_speech_token.py)
- [make_parquet_list.py](file://tools/make_parquet_list.py)
- [token2wav.py](file://runtime/triton_trtllm/token2wav.py)
- [fill_template.py](file://runtime/triton_trtllm/scripts/fill_template.py)
- [prepare_data.py](file://examples/libritts/cosyvoice/local/prepare_data.py)
- [run.sh](file://examples/grpo/cosyvoice2/run.sh)
- [infer_dataset.py](file://examples/grpo/cosyvoice2/infer_dataset.py)
</cite>

## 目录
1. [引言](#引言)
2. [说话人嵌入提取工具](#说话人嵌入提取工具)
3. [语音标记化工具](#语音标记化工具)
4. [大规模数据集管理工具](#大规模数据集管理工具)
5. [端到端推理流水线构建](#端到端推理流水线构建)
6. [命令行调用示例](#命令行调用示例)
7. [输出格式解析](#输出格式解析)

## 引言
CosyVoice提供了一系列高级工具，用于支持语音合成系统的开发、训练和推理。这些工具涵盖了从数据预处理到模型推理的完整流程，包括说话人嵌入提取、语音标记化、大规模数据集管理和端到端推理流水线构建等功能。本文档将详细介绍这些工具的使用场景和技术细节。

## 说话人嵌入提取工具

`extract_embedding.py` 是一个用于从参考音频中提取说话人嵌入向量的工具，主要用于语音克隆任务。该工具通过预训练的ONNX模型提取说话人特征，支持多线程并行处理以提高效率。

该工具首先读取音频文件并将其重采样到16kHz，然后使用Kaldi的FBank特征提取方法生成80维梅尔频谱特征。特征经过均值归一化处理后，输入到ONNX运行时的说话人嵌入模型中进行推理。对于每个音频片段（utterance），工具会生成对应的说话人嵌入向量，并将结果保存为PyTorch张量文件。

在说话人级别，工具会将同一说话人的多个音频片段的嵌入向量进行平均，生成该说话人的代表性嵌入向量。这种处理方式有助于提高语音克隆的稳定性和一致性。

**技术细节**：
- 输入音频重采样至16kHz
- 使用80维FBank特征
- 特征均值归一化
- ONNX模型推理
- 说话人级别嵌入平均

**Section sources**
- [extract_embedding.py](file://tools/extract_embedding.py#L1-L78)

## 语音标记化工具

`extract_speech_token.py` 负责将语音信号转换为离散的语音标记，支持离线预处理以加速后续的推理过程。该工具基于Whisper模型的对数梅尔频谱图提取技术，将音频转换为128维的对数梅尔频谱特征。

工具支持多线程并行处理，并针对长音频（超过30秒）进行了特殊处理，避免处理过长的音频片段。对于每个音频文件，工具会生成对应的语音标记序列，这些标记可以作为后续语言模型生成的条件输入。

语音标记化是连接声学模型和语言模型的关键步骤，它将连续的语音信号离散化为可处理的符号序列。通过离线预处理生成语音标记，可以在推理阶段显著减少计算开销，提高系统响应速度。

**技术细节**：
- 使用128维对数梅尔频谱
- 支持单声道音频转换
- 音频长度限制为30秒以内
- CUDA加速推理
- 离线批量处理

**Section sources**
- [extract_speech_token.py](file://tools/extract_speech_token.py#L1-L73)

## 大规模数据集管理工具

`make_parquet_list.py` 用于管理和组织大规模语音数据集，将原始数据转换为高效的Parquet格式存储。该工具支持多进程并行处理，能够快速处理大量音频文件。

工具读取多种数据源，包括音频文件、文本转录、说话人信息、嵌入向量和语音标记等，并将它们整合到一个结构化的Parquet文件中。每个Parquet文件包含固定数量的音频片段（默认1000个），便于后续的数据加载和批处理。

该工具还生成了多个索引文件，包括按音频片段和说话人组织的JSON映射文件，以及数据列表文件。这些索引文件使得数据访问更加高效，支持快速查找和检索特定的音频片段或说话人数据。

**功能特点**：
- 支持DPO（直接偏好优化）数据格式
- 支持指令微调数据格式
- 多进程并行处理
- 结构化数据存储
- 索引文件生成

**Section sources**
- [make_parquet_list.py](file://tools/make_parquet_list.py#L1-L137)

## 端到端推理流水线构建

结合 `token2wav.py` 和 `fill_template.py` 可以构建完整的端到端推理流水线。`token2wav.py` 是核心的语音生成模块，负责将语音标记序列转换为高质量的音频波形。

该模块包含多个组件：
- **Flow模型**：基于扩散机制的声学模型，负责从语音标记生成梅尔频谱
- **HiFT生成器**：高质量的声码器，将梅尔频谱转换为音频波形
- **说话人嵌入模型**：提取参考音频的说话人特征
- **语音标记化器**：将参考音频转换为语音标记

`fill_template.py` 是一个模板填充工具，用于配置Triton推理服务器的参数。它支持变量替换，可以动态修改配置文件中的队列大小、延迟等参数，适应不同的部署环境和性能需求。

**流水线工作流程**：
1. LLM生成语音标记序列
2. 提取参考音频的说话人嵌入
3. 将语音标记和说话人信息输入Flow模型
4. 生成梅尔频谱
5. 使用HiFT生成器合成音频波形

**Section sources**
- [token2wav.py](file://runtime/triton_trtllm/token2wav.py#L1-L336)
- [fill_template.py](file://runtime/triton_trtllm/scripts/fill_template.py#L1-L70)

## 命令行调用示例

以下是各个工具的典型命令行调用示例：

**提取说话人嵌入**：
```bash
python tools/extract_embedding.py \
  --dir /path/to/data \
  --onnx_path /path/to/campplus.onnx \
  --num_thread 8
```

**提取语音标记**：
```bash
python tools/extract_speech_token.py \
  --dir /path/to/data \
  --onnx_path /path/to/speech_tokenizer.onnx \
  --num_thread 8
```

**创建Parquet数据列表**：
```bash
python tools/make_parquet_list.py \
  --src_dir /path/to/source \
  --des_dir /path/to/destination \
  --num_utts_per_parquet 1000 \
  --num_processes 4 \
  --instruct
```

**填充配置模板**：
```bash
python runtime/triton_trtllm/scripts/fill_template.py \
  /path/to/config.pbtxt \
  "max_queue_size:100,max_queue_delay_microseconds:10000" \
  -i
```

**运行端到端推理**：
```bash
CUDA_VISIBLE_DEVICES=0 \
  python3 runtime/triton_trtllm/token2wav.py \
  --enable-trt \
  --model-dir ./CosyVoice2-0.5B \
  --batch-size 4 \
  --output-dir generated_wavs
```

**Section sources**
- [extract_embedding.py](file://tools/extract_embedding.py#L54-L58)
- [extract_speech_token.py](file://tools/extract_speech_token.py#L53-L57)
- [make_parquet_list.py](file://tools/make_parquet_list.py#L66-L87)
- [fill_template.py](file://runtime/triton_trtllm/scripts/fill_template.py#L58-L68)
- [token2wav.py](file://runtime/triton_trtllm/token2wav.py#L300-L307)

## 输出格式解析

各个工具生成的输出文件具有特定的格式和结构：

**说话人嵌入输出**：
- `utt2embedding.pt`：包含每个音频片段的嵌入向量
- `spk2embedding.pt`：包含每个说话人的平均嵌入向量
- 嵌入向量维度为192

**语音标记输出**：
- `utt2speech_token.pt`：包含每个音频片段的语音标记序列
- 标记序列为整数列表
- 空音频的标记序列为空列表

**Parquet数据输出**：
- `parquet_*.tar`：包含音频数据和相关特征的Parquet文件
- `utt2parquet_*.json`：音频片段到Parquet文件的映射
- `spk2parquet_*.json`：说话人到Parquet文件的映射
- `data.list`：所有Parquet文件的列表

**推理输出**：
- 生成的音频文件保存为WAV格式
- 采样率为24kHz
- 单声道输出
- 文件名与输入ID对应

**Section sources**
- [extract_embedding.py](file://tools/extract_embedding.py#L49-L50)
- [extract_speech_token.py](file://tools/extract_speech_token.py#L49)
- [make_parquet_list.py](file://tools/make_parquet_list.py#L57-L61)
- [token2wav.py](file://runtime/triton_trtllm/token2wav.py#L331)