import os
import time
import sys
import argparse
import logging
import re
import glob

# é…ç½®æ—¥å¿—æ ¼å¼ï¼Œç¡®ä¿æ˜¾ç¤ºæ¯«ç§’çº§æ—¶é—´æˆ³
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s.%(msecs)03d %(levelname)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logging.getLogger("matplotlib").setLevel(logging.WARNING)
from fastapi import FastAPI, Form, Request
from fastapi.responses import StreamingResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
import uvicorn
import numpy as np
import torch

# æ·»åŠ  Matcha-TTS è·¯å¾„
sys.path.append("third_party/Matcha-TTS")

# TensorRT-LLM ç›¸å…³
try:
    import tensorrt_llm
    from tensorrt_llm.runtime import ModelRunnerCpp
    from transformers import AutoTokenizer
    TRTLLM_AVAILABLE = True
except ImportError as e:
    TRTLLM_AVAILABLE = False
    logging.warning(f"TensorRT-LLM ä¸å¯ç”¨: {e}. å°†ä½¿ç”¨åŸå§‹ PyTorch æ¨ç†")

from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.file_utils import load_wav
import threading
import uuid as uuid_module

app = FastAPI()

# æ·»åŠ è¯·æ±‚è®¡æ—¶ä¸­é—´ä»¶ï¼ˆå¿…é¡»åœ¨ CORS ä¹‹å‰ï¼Œç¡®ä¿æœ€æ—©å¼€å§‹è®¡æ—¶ï¼‰
class TimingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # åœ¨è¯·æ±‚åˆšåˆ°è¾¾æ—¶ç«‹å³å¼€å§‹è®¡æ—¶
        request.state.start_time = time.time()
        response = await call_next(request)
        return response

app.add_middleware(TimingMiddleware)

# set cross region allowance
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")


# Chat template for TensorRT-LLM (CosyVoice2)
TRTLLM_CHAT_TEMPLATE = (
    "{%- for message in messages %}"
    "{%- if message['role'] == 'user' %}"
    "{{- '<|sos|>' + message['content'] + '<|task_id|>' }}"
    "{%- elif message['role'] == 'assistant' %}"
    "{{- message['content']}}"
    "{%- endif %}"
    "{%- endfor %}"
)


def convert_speech_tokens_to_str(speech_tokens):
    """å°† speech token IDs è½¬æ¢ä¸º <|s_XXXXX|> æ ¼å¼çš„å­—ç¬¦ä¸²"""
    if isinstance(speech_tokens, torch.Tensor):
        speech_tokens = speech_tokens.flatten().tolist()
    return ''.join([f"<|s_{token}|>" for token in speech_tokens])


def extract_speech_ids_from_str(speech_tokens_str_list):
    """ä» <|s_XXXXX|> æ ¼å¼çš„å­—ç¬¦ä¸²åˆ—è¡¨ä¸­æå– speech token IDs"""
    speech_ids = []
    for token_str in speech_tokens_str_list:
        if token_str.startswith('<|s_') and token_str.endswith('|>'):
            try:
                num_str = token_str[4:-2]
                num = int(num_str)
                speech_ids.append(num)
            except ValueError:
                logging.warning(f"æ— æ³•è§£æ speech token: {token_str}")
        # å¿½ç•¥å…¶ä»– tokenï¼ˆå¦‚ <|eos1|> ç­‰ï¼‰
    return speech_ids


class FastCosyVoice2:
    """é›†æˆ TensorRT-LLM çš„ CosyVoice2 æ¨ç†ç±»"""
    
    def __init__(self, cosyvoice_model, trtllm_engine_dir=None, trtllm_tokenizer_dir=None, use_trtllm=True):
        self.cosyvoice = cosyvoice_model
        self.use_trtllm = use_trtllm and TRTLLM_AVAILABLE
        self.trtllm_runner = None
        self.trtllm_tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # å­˜å‚¨åŸå§‹çš„ prompt_text å­—ç¬¦ä¸²ï¼Œç”¨äº TensorRT-LLM
        self.spk_prompt_text_raw = {}
        
        if self.use_trtllm:
            try:
                self._init_trtllm(trtllm_engine_dir, trtllm_tokenizer_dir)
                logging.info("âœ… TensorRT-LLM åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logging.error(f"TensorRT-LLM åˆå§‹åŒ–å¤±è´¥: {e}. å›é€€åˆ° PyTorch æ¨ç†")
                self.use_trtllm = False
    
    def _init_trtllm(self, engine_dir, tokenizer_dir):
        """åˆå§‹åŒ– TensorRT-LLM å¼•æ“"""
        if not engine_dir or not os.path.exists(engine_dir):
            raise ValueError(f"TensorRT-LLM å¼•æ“ç›®å½•ä¸å­˜åœ¨: {engine_dir}")
        
        runtime_rank = tensorrt_llm.mpi_rank()
        
        # åˆå§‹åŒ– tokenizer
        self.trtllm_tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        
        # è®¾ç½®æ­£ç¡®çš„ chat template
        if 'system' in self.trtllm_tokenizer.chat_template:
            self.trtllm_tokenizer.chat_template = TRTLLM_CHAT_TEMPLATE
            logging.info("å·²è®¾ç½® CosyVoice2 ä¸“ç”¨ chat template")
        
        # EOS token ID
        self.eos_token_id = self.trtllm_tokenizer.convert_tokens_to_ids("<|eos1|>")
        
        # åˆå§‹åŒ– TensorRT-LLM ModelRunner
        runner_kwargs = dict(
            engine_dir=engine_dir,
            rank=runtime_rank,
            max_output_len=2048,
            enable_context_fmha_fp32_acc=False,
            max_batch_size=1,
            max_input_len=512,
            kv_cache_free_gpu_memory_fraction=0.6,
            cuda_graph_mode=False,
            gather_generation_logits=False,
        )
        self.trtllm_runner = ModelRunnerCpp.from_dir(**runner_kwargs)
        logging.info(f"TensorRT-LLM å¼•æ“å·²åŠ è½½: {engine_dir}")
    
    def _prepare_llm_input(self, tts_text, prompt_text, prompt_speech_tokens):
        """å‡†å¤‡ LLM è¾“å…¥ï¼ˆä½¿ç”¨ chat templateï¼‰
        
        Args:
            tts_text: è¦åˆæˆçš„ç›®æ ‡æ–‡æœ¬ï¼ˆåŸå§‹å­—ç¬¦ä¸²ï¼‰
            prompt_text: æç¤ºæ–‡æœ¬ï¼ˆåŸå§‹å­—ç¬¦ä¸²ï¼‰
            prompt_speech_tokens: æç¤ºè¯­éŸ³çš„ speech token IDsï¼ˆtensor æˆ– listï¼‰
        
        Returns:
            input_ids: tokenized åçš„è¾“å…¥ tensor
        """
        # 1. æ‹¼æ¥å®Œæ•´æ–‡æœ¬ï¼šprompt_text + tts_text
        full_text = prompt_text + tts_text
        
        # 2. å°† prompt_speech_tokens è½¬æ¢ä¸º <|s_XXXXX|> æ ¼å¼å­—ç¬¦ä¸²
        prompt_speech_str = convert_speech_tokens_to_str(prompt_speech_tokens)
        
        # 3. æ„å»º chat æ ¼å¼
        chat = [
            {"role": "user", "content": full_text},
            {"role": "assistant", "content": prompt_speech_str}
        ]
        
        # 4. ä½¿ç”¨ chat template è¿›è¡Œ tokenization
        input_ids = self.trtllm_tokenizer.apply_chat_template(
            chat,
            tokenize=True,
            return_tensors='pt',
            continue_final_message=True  # ç»§ç»­ç”Ÿæˆ assistant çš„å›å¤
        )
        
        return input_ids
    
    def _trtllm_generate_streaming(self, input_ids):
        """ä½¿ç”¨ TensorRT-LLM æµå¼ç”Ÿæˆ speech tokens
        
        Args:
            input_ids: tokenized è¾“å…¥ tensor
        
        Yields:
            Tuple[List[int], bool]: (å½“å‰ç´¯ç§¯çš„ speech_ids, æ˜¯å¦å®Œæˆ)
        """
        try:
            input_length = input_ids.shape[1]
            
            # TensorRT-LLM æµå¼ç”Ÿæˆ
            outputs_iter = self.trtllm_runner.generate(
                batch_input_ids=[input_ids[0]],
                max_new_tokens=2048,
                end_id=self.eos_token_id,
                pad_id=self.eos_token_id,
                temperature=0.8,
                top_k=50,
                top_p=0.95,
                repetition_penalty=1.1,
                streaming=True,  # å¯ç”¨æµå¼ç”Ÿæˆ
                output_sequence_lengths=True,
                return_dict=True,
            )
            
            # è¿­ä»£æµå¼è¾“å‡º
            for outputs in outputs_iter:
                torch.cuda.synchronize()
                
                # æå–ç”Ÿæˆçš„ token IDs
                output_ids = outputs["output_ids"]
                sequence_lengths = outputs["sequence_lengths"]
                
                # è·å–å®é™…ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆæ’é™¤è¾“å…¥ï¼‰
                actual_length = sequence_lengths[0][0].item()
                generated_ids = output_ids[0][0][input_length:actual_length].tolist()
                
                # å°† token IDs è§£ç ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åè§£æ <|s_XXXXX|> æ ¼å¼
                generated_tokens_str = self.trtllm_tokenizer.batch_decode(
                    [[tid] for tid in generated_ids],
                    skip_special_tokens=False
                )
                
                # æå–çœŸå®çš„ speech token IDs
                speech_ids = extract_speech_ids_from_str(generated_tokens_str)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªå“åº”
                is_final = outputs.get("finished", False)
                if isinstance(is_final, torch.Tensor):
                    is_final = is_final.item()
                
                yield speech_ids, is_final
                
                if is_final:
                    break
            
        except Exception as e:
            logging.error(f"TensorRT-LLM æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _trtllm_generate(self, input_ids):
        """ä½¿ç”¨ TensorRT-LLM ç”Ÿæˆ speech tokensï¼ˆéæµå¼ï¼Œç”¨äºå›é€€ï¼‰
        
        Args:
            input_ids: tokenized è¾“å…¥ tensor
        
        Returns:
            speech_ids: ç”Ÿæˆçš„ speech token IDs åˆ—è¡¨
        """
        try:
            input_length = input_ids.shape[1]
            
            # TensorRT-LLM ç”Ÿæˆ
            outputs = self.trtllm_runner.generate(
                batch_input_ids=[input_ids[0]],
                max_new_tokens=2048,
                end_id=self.eos_token_id,
                pad_id=self.eos_token_id,
                temperature=0.8,
                top_k=50,
                top_p=0.95,
                repetition_penalty=1.1,
                streaming=False,
                output_sequence_lengths=True,
                return_dict=True,
            )
            
            torch.cuda.synchronize()
            
            # æå–ç”Ÿæˆçš„ token IDs
            output_ids = outputs["output_ids"]
            sequence_lengths = outputs["sequence_lengths"]
            
            # è·å–å®é™…ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆæ’é™¤è¾“å…¥ï¼‰
            actual_length = sequence_lengths[0][0].item()
            generated_ids = output_ids[0][0][input_length:actual_length].tolist()
            
            # å°† token IDs è§£ç ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åè§£æ <|s_XXXXX|> æ ¼å¼
            generated_tokens_str = self.trtllm_tokenizer.batch_decode(
                [[tid] for tid in generated_ids],
                skip_special_tokens=False
            )
            
            # æå–çœŸå®çš„ speech token IDs
            speech_ids = extract_speech_ids_from_str(generated_tokens_str)
            
            logging.info(f"TensorRT-LLM ç”Ÿæˆäº† {len(speech_ids)} ä¸ª speech tokens")
            return speech_ids
            
        except Exception as e:
            logging.error(f"TensorRT-LLM ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def inference_zero_shot(self, text, prompt_text, prompt_speech_16k, zero_shot_spk_id='', stream=True):
        """é›¶æ ·æœ¬æ¨ç†ï¼ˆé›†æˆ TensorRT-LLM æµå¼ç”Ÿæˆï¼‰"""
        if not self.use_trtllm:
            # å›é€€åˆ°åŸå§‹ CosyVoice2 æ¨ç†
            for output in self.cosyvoice.inference_zero_shot(
                text, prompt_text, prompt_speech_16k, 
                zero_shot_spk_id=zero_shot_spk_id, 
                stream=stream
            ):
                yield output
            return
        
        # ä½¿ç”¨ TensorRT-LLM åŠ é€Ÿçš„æ¨ç†æµç¨‹ï¼ˆæµå¼ç”Ÿæˆ + æµå¼ token2wavï¼‰
        try:
            # 1. è·å– speaker infoï¼ˆåŒ…å« prompt_speech_tokensï¼‰
            spk_info = self.cosyvoice.frontend.spk2info.get(zero_shot_spk_id)
            if spk_info is None:
                raise ValueError(f"Speaker {zero_shot_spk_id} ä¸å­˜åœ¨")
            
            # 2. è·å–åŸå§‹çš„ prompt_text å­—ç¬¦ä¸²
            prompt_text_raw = self.spk_prompt_text_raw.get(zero_shot_spk_id, '')
            if not prompt_text_raw:
                logging.warning(f"Speaker {zero_shot_spk_id} ç¼ºå°‘åŸå§‹ prompt_textï¼Œå›é€€åˆ° PyTorch æ¨ç†")
                raise ValueError("ç¼ºå°‘åŸå§‹ prompt_text")
            
            # 3. è·å– spk_info ä¸­çš„æ•°æ®
            llm_prompt_speech_token = spk_info['llm_prompt_speech_token']
            flow_prompt_speech_token = spk_info['flow_prompt_speech_token']
            prompt_speech_feat = spk_info['prompt_speech_feat']
            flow_embedding = spk_info['flow_embedding']
            
            # 4. å‡†å¤‡ LLM è¾“å…¥ï¼ˆä½¿ç”¨åŸå§‹å­—ç¬¦ä¸² + chat templateï¼‰
            input_ids = self._prepare_llm_input(text, prompt_text_raw, llm_prompt_speech_token)
            
            logging.info(f"[FastTTS] TensorRT-LLM è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")
            
            # 5. åˆå§‹åŒ–æµå¼å‚æ•°
            this_uuid = str(uuid_module.uuid1())
            model = self.cosyvoice.model
            model.hift_cache_dict[this_uuid] = None
            
            token_hop_len = 25  # æ¯å—å¤„ç†çš„ token æ•°
            pre_lookahead_len = model.flow.pre_lookahead_len  # å‰ç»é•¿åº¦ (3)
            
            # è®¡ç®— prompt_token_padï¼ˆå¯¹é½åˆ° token_hop_len çš„å€æ•°ï¼‰
            prompt_token_pad = int(np.ceil(flow_prompt_speech_token.shape[1] / token_hop_len) * token_hop_len - flow_prompt_speech_token.shape[1])
            
            token_offset = 0
            chunk_idx = 0
            first_chunk_tokens_needed = token_hop_len + prompt_token_pad + pre_lookahead_len  # ~28
            
            logging.info(f"[æµå¼ç”Ÿæˆ] å¼€å§‹æµå¼ç”Ÿæˆ+token2wav: first_chunk_needed={first_chunk_tokens_needed}")
            
            # 6. æµå¼ç”Ÿæˆ + æµå¼ token2wav
            speech_tokens = []
            generation_done = False
            
            for current_tokens, is_final in self._trtllm_generate_streaming(input_ids):
                speech_tokens = current_tokens
                generation_done = is_final
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ tokens ç”Ÿæˆç¬¬ä¸€å—éŸ³é¢‘
                while True:
                    this_token_hop_len = token_hop_len + prompt_token_pad if token_offset == 0 else token_hop_len
                    tokens_needed = token_offset + this_token_hop_len + pre_lookahead_len
                    
                    if tokens_needed <= len(speech_tokens):
                        # æœ‰è¶³å¤Ÿçš„ tokensï¼Œç”Ÿæˆä¸€å—éŸ³é¢‘
                        chunk_start_time = time.time()
                        
                        this_tts_speech_token = torch.tensor(
                            speech_tokens[:tokens_needed]
                        ).unsqueeze(0)
                        
                        tts_speech = model.token2wav(
                            token=this_tts_speech_token,
                            prompt_token=flow_prompt_speech_token,
                            prompt_feat=prompt_speech_feat,
                            embedding=flow_embedding,
                            token_offset=token_offset,
                            uuid=this_uuid,
                            stream=True,
                            finalize=False,
                            speed=1.0
                        )
                        
                        chunk_time = (time.time() - chunk_start_time) * 1000
                        logging.info(f"[æµå¼token2wav] å—{chunk_idx}: offset={token_offset}, tokens_in={tokens_needed}, total_generated={len(speech_tokens)}, è€—æ—¶={chunk_time:.1f}ms")
                        
                        token_offset += this_token_hop_len
                        chunk_idx += 1
                        yield {'tts_speech': tts_speech.cpu()}
                    else:
                        # ä¸å¤Ÿ tokensï¼Œç­‰å¾…æ›´å¤šç”Ÿæˆ
                        break
                
                if generation_done:
                    break
            
            # 7. å¤„ç†å‰©ä½™çš„ tokensï¼ˆæœ€åä¸€å—ï¼‰
            if token_offset < len(speech_tokens):
                chunk_start_time = time.time()
                
                this_tts_speech_token = torch.tensor(speech_tokens).unsqueeze(0)
                
                tts_speech = model.token2wav(
                    token=this_tts_speech_token,
                    prompt_token=flow_prompt_speech_token,
                    prompt_feat=prompt_speech_feat,
                    embedding=flow_embedding,
                    token_offset=token_offset,
                    uuid=this_uuid,
                    stream=True,
                    finalize=True,
                    speed=1.0
                )
                
                chunk_time = (time.time() - chunk_start_time) * 1000
                logging.info(f"[æµå¼token2wav] æœ€ç»ˆå—{chunk_idx}: offset={token_offset}, tokens_in={len(speech_tokens)}, è€—æ—¶={chunk_time:.1f}ms (finalize)")
                yield {'tts_speech': tts_speech.cpu()}
            
            logging.info(f"[FastTTS] æµå¼ç”Ÿæˆå®Œæˆ: å…± {len(speech_tokens)} ä¸ª speech tokens, {chunk_idx + 1} ä¸ªéŸ³é¢‘å—")
            
            # æ¸…ç†ç¼“å­˜
            if this_uuid in model.hift_cache_dict:
                model.hift_cache_dict.pop(this_uuid)
            
        except Exception as e:
            logging.error(f"TensorRT-LLM æ¨ç†å¤±è´¥: {e}ï¼Œå›é€€åˆ° PyTorch æ¨ç†")
            import traceback
            traceback.print_exc()
            # å›é€€åˆ°åŸå§‹æ¨ç†
            for output in self.cosyvoice.inference_zero_shot(
                text, prompt_text, prompt_speech_16k,
                zero_shot_spk_id=zero_shot_spk_id,
                stream=stream
            ):
                yield output
    
    def list_available_spks(self):
        """è·å–å¯ç”¨è¯´è¯äººåˆ—è¡¨"""
        return self.cosyvoice.list_available_spks()
    
    def add_zero_shot_spk(self, prompt_text, prompt_speech_16k, zero_shot_spk_id):
        """æ·»åŠ é›¶æ ·æœ¬è¯´è¯äºº"""
        # ä¿å­˜åŸå§‹çš„ prompt_text å­—ç¬¦ä¸²ï¼ˆç”¨äº TensorRT-LLMï¼‰
        self.spk_prompt_text_raw[zero_shot_spk_id] = prompt_text
        return self.cosyvoice.add_zero_shot_spk(prompt_text, prompt_speech_16k, zero_shot_spk_id)
    
    def save_spkinfo(self):
        """ä¿å­˜è¯´è¯äººä¿¡æ¯"""
        return self.cosyvoice.save_spkinfo()


def generate_data(model_output, request_start_time):
    """ç”ŸæˆéŸ³é¢‘æ•°æ®æµï¼Œå¯¹è¾“å‡ºè¿›è¡Œå‰Šæ³¢å¤„ç†é˜²æ­¢çˆ†éŸ³"""
    is_first = True
    chunk_count = 0
    
    for i in model_output:
        if is_first:
            first_chunk_time = time.time()
            ttfb = (first_chunk_time - request_start_time) * 1000
            logging.info(f"[TTSç»Ÿè®¡] é¦–åŒ…ç”Ÿæˆå®Œæ¯•! æœåŠ¡ç«¯TTFB: {ttfb:.2f}ms")
            is_first = False

        tts_speech = i["tts_speech"].numpy()
        
        # 2. è¾“å‡ºç«¯å‰Šæ³¢ï¼šé˜²æ­¢ float -> int16 è½¬æ¢æ—¶çš„æ•´æ•°æº¢å‡º
        tts_speech = np.clip(tts_speech, -1.0, 1.0)
        
        # è½¬æ¢ä¸º int16 æ ¼å¼
        tts_audio = (tts_speech * 32767).astype(np.int16).tobytes()
        chunk_count += 1
        yield tts_audio
    
    total_time = (time.time() - request_start_time) * 1000
    logging.info(f"[TTSç»Ÿè®¡] æµå¼ä¼ è¾“ç»“æŸ. æ€»è€—æ—¶: {total_time:.2f}ms, å…±å‘é€ {chunk_count} ä¸ªæ•°æ®å—")


def load_speakers_from_directory(speaker_dir="asset/speakers"):
    """ä»ç›®å½•åŠ è½½æ‰€æœ‰è¯´è¯äºº"""
    speakers = {}
    
    if not os.path.exists(speaker_dir):
        logging.warning(f"è¯´è¯äººç›®å½• {speaker_dir} ä¸å­˜åœ¨")
        return speakers
    
    wav_files = glob.glob(os.path.join(speaker_dir, "*.wav"))
    
    for wav_path in wav_files:
        filename = os.path.basename(wav_path)
        # è§£ææ–‡ä»¶åæ ¼å¼ï¼š[è¯´è¯äººåç§°]æ–‡æœ¬å†…å®¹.wav
        match = re.match(r'\[(.+?)\](.+)\.wav$', filename)
        
        if match:
            speaker_name = match.group(1)
            prompt_text = match.group(2)
            
            try:
                prompt_speech_16k = load_wav(wav_path, 16000)
                
                # è¾“å…¥ç«¯å½’ä¸€åŒ–
                max_val = torch.abs(prompt_speech_16k).max().item()
                target_peak = 0.95
                
                if max_val > target_peak:
                    logging.warning(f"è¯´è¯äºº {speaker_name} éŸ³é¢‘å³°å€¼ {max_val:.4f} è¶…å‡ºå®‰å…¨èŒƒå›´ï¼Œå½’ä¸€åŒ–åˆ° {target_peak}")
                    prompt_speech_16k = (prompt_speech_16k / max_val) * target_peak
                
                speakers[speaker_name] = {
                    'prompt_text': prompt_text,
                    'prompt_speech_16k': prompt_speech_16k,
                    'wav_path': wav_path
                }
                logging.info(f"åŠ è½½è¯´è¯äºº: {speaker_name}")
            except Exception as e:
                logging.error(f"åŠ è½½è¯´è¯äºº {speaker_name} å¤±è´¥: {e}")
        else:
            logging.warning(f"æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡: {filename}")
    
    return speakers


@app.get("/")
async def index():
    """ä¸»é¡µè·¯ç”±ï¼Œè¿”å›å‰ç«¯é¡µé¢"""
    index_path = os.path.join(os.path.dirname(__file__), "static", "index.html")
    if os.path.exists(index_path):
        return FileResponse(index_path)
    return {"message": "FastCosyVoice TTS Server (TensorRT-LLM Accelerated) is running. Visit /static/index.html for the web interface."}


@app.get("/api/speakers")
async def get_speakers():
    """è·å–æ‰€æœ‰å¯ç”¨çš„è¯´è¯äººåˆ—è¡¨"""
    try:
        speakers = fast_cosyvoice.list_available_spks()
        return JSONResponse(content={"speakers": speakers})
    except Exception as e:
        logging.error(f"è·å–è¯´è¯äººåˆ—è¡¨å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/tts")
async def inference_zero_shot(request: Request, text: str = Form(), speaker: str = Form(default="")):
    """æ–‡æœ¬è½¬è¯­éŸ³æ¥å£ï¼ˆé›†æˆ TensorRT-LLM åŠ é€Ÿï¼‰"""
    # ä½¿ç”¨ä¸­é—´ä»¶è®°å½•çš„å¼€å§‹æ—¶é—´ï¼Œç¡®ä¿ä¸å‰ç«¯å¯¹é½
    request_start_time = request.state.start_time
    logging.info(f"[FastTTSè¯·æ±‚] æ”¶åˆ°è¯·æ±‚: text='{text[:50] if len(text) > 50 else text}', speaker='{speaker}'")

    try:
        # é»˜è®¤ä½¿ç”¨jokè€å¸ˆï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªè¯´è¯äºº
        default_speaker = "jokè€å¸ˆ" if "jokè€å¸ˆ" in speakers_data else (list(speakers_data.keys())[0] if speakers_data else "")
        selected_speaker = speaker if speaker else default_speaker
        
        if not selected_speaker:
            return JSONResponse(content={"error": "æ²¡æœ‰å¯ç”¨çš„è¯´è¯äºº"}, status_code=400)
        
        logging.info(f"[FastTTSæ¨ç†] å¼€å§‹æ¨ç†, è¯´è¯äºº: {selected_speaker}")

        # ä½¿ç”¨ FastCosyVoice2 è¿›è¡Œæ¨ç†ï¼ˆè‡ªåŠ¨é€‰æ‹© TensorRT-LLM æˆ– PyTorchï¼‰
        model_output = fast_cosyvoice.inference_zero_shot(
            text, "", None, 
            zero_shot_spk_id=selected_speaker,
            stream=True
        )
        return StreamingResponse(generate_data(model_output, request_start_time))
    except Exception as e:
        logging.error(f"FastTTSæ¨ç†å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=6008, help="æœåŠ¡ç«¯å£")
    parser.add_argument(
        "--model_dir",
        type=str,
        default="pretrained_models/CosyVoice2-0.5B",
        help="æ¨¡å‹æœ¬åœ°è·¯å¾„æˆ– modelscope ä»“åº“ id",
    )
    parser.add_argument(
        "--trtllm_engine_dir",
        type=str,
        default="runtime/triton_trtllm/trt_engines_bfloat16",
        help="TensorRT-LLM å¼•æ“ç›®å½•",
    )
    parser.add_argument(
        "--trtllm_tokenizer_dir",
        type=str,
        default="runtime/triton_trtllm/cosyvoice2_llm",
        help="TensorRT-LLM tokenizer ç›®å½•",
    )
    parser.add_argument(
        "--speaker_dir",
        type=str,
        default="asset/speakers",
        help="è¯´è¯äººéŸ³é¢‘æ–‡ä»¶ç›®å½•",
    )
    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–åŸºç¡€ CosyVoice2 æ¨¡å‹
        logging.info("=" * 60)
        logging.info("å¯åŠ¨ FastCosyVoice TTS Server")
        logging.info(f"æ¨¡å‹ç›®å½•: {args.model_dir}")
        logging.info(f"TensorRT-LLM å¼•æ“ç›®å½•: {args.trtllm_engine_dir}")
        logging.info(f"TensorRT-LLM Tokenizer: {args.trtllm_tokenizer_dir}")
        logging.info("=" * 60)
        
        # åŠ è½½åŸå§‹ CosyVoice2 æ¨¡å‹
        cosyvoice = CosyVoice2(args.model_dir, load_jit=True, load_trt=True, fp16=True)
        
        # åˆ›å»º FastCosyVoice2 å®ä¾‹ï¼ˆé›†æˆ TensorRT-LLMï¼‰
        fast_cosyvoice = FastCosyVoice2(
            cosyvoice_model=cosyvoice,
            trtllm_engine_dir=args.trtllm_engine_dir,
            trtllm_tokenizer_dir=args.trtllm_tokenizer_dir,
        )
        
        logging.info("âœ… FastCosyVoice2 åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ TensorRT-LLM åŠ é€Ÿ")

    except Exception as e:
        raise TypeError(f"å¯¼å…¥{args.model_dir}å¤±è´¥ï¼Œæ¨¡å‹ç±»å‹æœ‰è¯¯ï¼é”™è¯¯: {e}")

    # åŠ è½½æ‰€æœ‰è¯´è¯äºº
    print("æ­£åœ¨åŠ è½½è¯´è¯äºº...")
    speakers_data = load_speakers_from_directory(args.speaker_dir)
    
    if not speakers_data:
        print(f"è­¦å‘Šï¼šæœªåœ¨ {args.speaker_dir} ç›®å½•æ‰¾åˆ°ä»»ä½•è¯´è¯äººæ–‡ä»¶")
    else:
        print(f"æˆåŠŸåŠ è½½ {len(speakers_data)} ä¸ªè¯´è¯äºº")
        
        # å°†æ‰€æœ‰è¯´è¯äººæ·»åŠ åˆ°æ¨¡å‹
        for speaker_name, speaker_info in speakers_data.items():
            try:
                fast_cosyvoice.add_zero_shot_spk(
                    speaker_info['prompt_text'],
                    speaker_info['prompt_speech_16k'],
                    speaker_name
                )
                print(f"  âœ“ {speaker_name}")
            except Exception as e:
                print(f"  âœ— {speaker_name}: {e}")
        
        # ä¿å­˜è¯´è¯äººä¿¡æ¯
        try:
            fast_cosyvoice.save_spkinfo()
            print("è¯´è¯äººä¿¡æ¯å·²ä¿å­˜")
        except Exception as e:
            print(f"ä¿å­˜è¯´è¯äººä¿¡æ¯å¤±è´¥: {e}")
    
    # æ¨¡å‹é¢„çƒ­
    print("\næ­£åœ¨é¢„çƒ­æ¨¡å‹...")
    if speakers_data:
        warmup_speaker = "jokè€å¸ˆ" if "jokè€å¸ˆ" in speakers_data else list(speakers_data.keys())[0]
        print(f"ä½¿ç”¨ '{warmup_speaker}' è¿›è¡Œé¢„çƒ­")
        warmup_texts = [
            'æ”¶åˆ°å¥½å‹ä»è¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œ', 
        ]
        
        for t in warmup_texts:
            try:
                for _ in fast_cosyvoice.inference_zero_shot(
                        t, "", None, zero_shot_spk_id=warmup_speaker, stream=True):
                    pass
            except Exception as e:
                print(f"é¢„çƒ­å¤±è´¥: {e}")
                break
    
    print("é¢„çƒ­å®Œæ¯•\n")
    print("=" * 60)
    print(f"ğŸš€ FastCosyVoice TTS Server å¯åŠ¨åœ¨ç«¯å£ {args.port}")

    # é…ç½® uvicorn ä»¥æ”¯æŒ HTTP keep-alive
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=args.port,
        timeout_keep_alive=60,  # keep-alive è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        limit_concurrency=100,  # æœ€å¤§å¹¶å‘è¿æ¥æ•°
        backlog=2048,  # TCP backlog é˜Ÿåˆ—å¤§å°
    )
