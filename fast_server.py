import os
import time
import sys
import argparse
import logging
import re
import glob
import json
from functools import partial
import inflect

# é…ç½®æ—¥å¿—æ ¼å¼ï¼Œç¡®ä¿æ˜¾ç¤ºæ¯«ç§’çº§æ—¶é—´æˆ³
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s.%(msecs)03d %(levelname)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logging.getLogger("matplotlib").setLevel(logging.WARNING)

from fastapi import FastAPI, Form, Request
from fastapi.responses import StreamingResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
import uvicorn
import numpy as np
import torch

# æ·»åŠ  Matcha-TTS è·¯å¾„
sys.path.append("third_party/Matcha-TTS")

# TensorRT-LLM ç›¸å…³
try:
    import tensorrt_llm
    from tensorrt_llm.runtime import ModelRunnerCpp
    from transformers import AutoTokenizer
    TRTLLM_AVAILABLE = True
except ImportError as e:
    TRTLLM_AVAILABLE = False
    logging.warning(f"TensorRT-LLM ä¸å¯ç”¨: {e}. å°†ä½¿ç”¨åŸå§‹ PyTorch æ¨ç†")

from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.file_utils import load_wav
from cosyvoice.utils.frontend_utils import (
    contains_chinese, replace_blank, replace_corner_mark, 
    remove_bracket, spell_out_number, split_paragraph, is_only_punctuation
)
import uuid as uuid_module

# å¯¼å…¥ ttsfrd æ¨¡å—ï¼ˆç”¨äºæ–‡æœ¬è§„èŒƒåŒ–ï¼‰
try:
    import ttsfrd
    USE_TTSFRD = True
    logging.info("å·²å¯¼å…¥ ttsfrd æ¨¡å—ç”¨äºæ–‡æœ¬è§„èŒƒåŒ–")
except ImportError:
    USE_TTSFRD = False
    logging.warning("ttsfrd ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ wetext è¿›è¡Œæ–‡æœ¬è§„èŒƒåŒ–")
    try:
        from wetext import Normalizer as ZhNormalizer
        from wetext import Normalizer as EnNormalizer
    except ImportError:
        logging.warning("wetext ä¹Ÿä¸å¯ç”¨ï¼Œæ–‡æœ¬è§„èŒƒåŒ–åŠŸèƒ½å°†å—é™")

app = FastAPI()

# æ·»åŠ è¯·æ±‚è®¡æ—¶ä¸­é—´ä»¶ï¼ˆå¿…é¡»åœ¨ CORS ä¹‹å‰ï¼Œç¡®ä¿æœ€æ—©å¼€å§‹è®¡æ—¶ï¼‰
class TimingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # åœ¨è¯·æ±‚åˆšåˆ°è¾¾æ—¶ç«‹å³å¼€å§‹è®¡æ—¶
        request.state.start_time = time.perf_counter()
        response = await call_next(request)
        return response

app.add_middleware(TimingMiddleware)

# è®¾ç½®åŒæºç­–ç•¥
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")


# Chat template for TensorRT-LLM (CosyVoice2)
TRTLLM_CHAT_TEMPLATE = (
    "{%- for message in messages %}"
    "{%- if message['role'] == 'user' %}"
    "{{- '<|sos|>' + message['content'] + '<|task_id|>' }}"
    "{%- elif message['role'] == 'assistant' %}"
    "{{- message['content']}}"
    "{%- endif %}"
    "{%- endfor %}"
)


class TextNormalizer:
    """æ–‡æœ¬è§„èŒƒåŒ–å·¥å…·ç±»ï¼Œå¤ç”¨ CosyVoice frontend çš„æˆç†Ÿå®ç°"""
    
    def __init__(self, tokenizer, use_ttsfrd=True):
        self.tokenizer = tokenizer
        self.use_ttsfrd = use_ttsfrd
        
        if self.use_ttsfrd:
            self.frd = ttsfrd.TtsFrontendEngine()
            # åˆå§‹åŒ– ttsfrd èµ„æºï¼ˆä½¿ç”¨é¡¹ç›®ä¸­çš„èµ„æºç›®å½•ï¼‰
            resource_dir = os.path.join(os.path.dirname(__file__), 'pretrained_models/CosyVoice-ttsfrd/resource')
            if os.path.exists(resource_dir):
                if not self.frd.initialize(resource_dir):
                    logging.warning(f"ttsfrd åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ wetext")
                    self.use_ttsfrd = False
                    self._init_wetext()
                else:
                    self.frd.set_lang_type('pinyinvg')
                    logging.info("âœ… ttsfrd åˆå§‹åŒ–æˆåŠŸ")
            else:
                logging.warning(f"ttsfrd èµ„æºç›®å½•ä¸å­˜åœ¨: {resource_dir}ï¼Œå°†ä½¿ç”¨ wetext")
                self.use_ttsfrd = False
                self._init_wetext()
        else:
            self._init_wetext()
    
    def _init_wetext(self):
        """åˆå§‹åŒ– wetext è§„èŒƒåŒ–å™¨ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        if 'ZhNormalizer' in globals():
            self.zh_tn_model = ZhNormalizer(remove_erhua=False)
            self.en_tn_model = EnNormalizer()
            self.inflect_parser = inflect.engine()
            logging.info("ä½¿ç”¨ wetext ä½œä¸ºæ–‡æœ¬è§„èŒƒåŒ–å·¥å…·")
        else:
            logging.warning("wetext ä¸å¯ç”¨ï¼Œå°†è·³è¿‡æ–‡æœ¬è§„èŒƒåŒ–")
    
    def normalize_and_split(self, text, token_max_n=80, token_min_n=60):
        """æ–‡æœ¬è§„èŒƒåŒ–ä¸æ™ºèƒ½åˆ†æ®µï¼ˆå¤ç”¨ frontend.text_normalize é€»è¾‘ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            token_max_n: æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ 80ï¼‰
            token_min_n: æœ€å° token æ•°ï¼ˆé»˜è®¤ 60ï¼‰
        
        Returns:
            List[str]: è§„èŒƒåŒ–å¹¶åˆ‡åˆ†åçš„æ–‡æœ¬æ®µè½åˆ—è¡¨
        """
        if not text or text.strip() == '':
            return []
        
        text = text.strip()
        
        # ä½¿ç”¨ ttsfrd è¿›è¡Œæ–‡æœ¬è§„èŒƒåŒ–
        if self.use_ttsfrd:
            try:
                result = self.frd.do_voicegen_frd(text)
                texts = [i["text"] for i in json.loads(result)["sentences"]]
                text = ''.join(texts)
            except Exception as e:
                logging.warning(f"ttsfrd å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
        else:
            # ä½¿ç”¨ wetext è¿›è¡Œè§„èŒƒåŒ–ï¼ˆä¸ frontend.py é€»è¾‘ä¸€è‡´ï¼‰
            if 'zh_tn_model' in dir(self):
                if contains_chinese(text):
                    text = self.zh_tn_model.normalize(text)
                    text = text.replace("\n", "")
                    text = replace_blank(text)
                    text = replace_corner_mark(text)
                    text = text.replace(".", "ã€‚")
                    text = text.replace(" - ", "ï¼Œ")
                    text = remove_bracket(text)
                    text = re.sub(r'[ï¼Œ,ã€]+$', 'ã€‚', text)
                else:
                    text = self.en_tn_model.normalize(text)
                    text = spell_out_number(text, self.inflect_parser)
        
        # ä½¿ç”¨ split_paragraph è¿›è¡Œæ™ºèƒ½åˆ†æ®µï¼ˆä¸ frontend.py é€»è¾‘ä¸€è‡´ï¼‰
        tokenize_fn = partial(self.tokenizer.encode, allowed_special='all')
        
        if contains_chinese(text):
            texts = list(split_paragraph(
                text, tokenize_fn, "zh", 
                token_max_n=token_max_n,
                token_min_n=token_min_n, 
                merge_len=20, 
                comma_split=False
            ))
        else:
            texts = list(split_paragraph(
                text, tokenize_fn, "en", 
                token_max_n=token_max_n,
                token_min_n=token_min_n, 
                merge_len=20, 
                comma_split=False
            ))
        
        # è¿‡æ»¤çº¯æ ‡ç‚¹æ®µè½
        texts = [i for i in texts if not is_only_punctuation(i)]
        
        if texts:
            logging.info(f"[æ–‡æœ¬è§„èŒƒåŒ–] åŸå§‹é•¿åº¦: {len(text)} å­—ç¬¦ â†’ åˆ†æˆ {len(texts)} æ®µ")
            for idx, seg in enumerate(texts):
                logging.info(f"  æ®µ{idx+1}: {len(seg)}å­—ç¬¦ - '{seg[:30]}{'...' if len(seg) > 30 else ''}'")
        
        return texts


def convert_speech_tokens_to_str(speech_tokens):
    """å°† speech token IDs è½¬æ¢ä¸º <|s_XXXXX|> æ ¼å¼çš„å­—ç¬¦ä¸²"""
    if isinstance(speech_tokens, torch.Tensor):
        speech_tokens = speech_tokens.flatten().tolist()
    return ''.join([f"<|s_{token}|>" for token in speech_tokens])


def extract_speech_ids_from_str(speech_tokens_str_list):
    """ä» <|s_XXXXX|> æ ¼å¼çš„å­—ç¬¦ä¸²åˆ—è¡¨ä¸­æå– speech token IDs"""
    speech_ids = []
    for token_str in speech_tokens_str_list:
        if token_str.startswith('<|s_') and token_str.endswith('|>'):
            try:
                num_str = token_str[4:-2]
                num = int(num_str)
                speech_ids.append(num)
            except ValueError:
                logging.warning(f"æ— æ³•è§£æ speech token: {token_str}")
        # å¿½ç•¥å…¶ä»– tokenï¼ˆå¦‚ <|eos1|> ç­‰ï¼‰
    return speech_ids


class FastCosyVoice2:
    """é›†æˆ TensorRT-LLM çš„ CosyVoice2 æ¨ç†ç±»"""
    
    def __init__(self, cosyvoice_model, trtllm_engine_dir, trtllm_tokenizer_dir, spk2info_path=None):
        self.cosyvoice : CosyVoice2 = cosyvoice_model
        self.trtllm_runner = None
        self.trtllm_tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.spk2info_path = spk2info_path  # ä¿å­˜ spk2info.pt çš„è·¯å¾„
        
        # å­˜å‚¨åŸå§‹çš„ prompt_text å­—ç¬¦ä¸²ï¼Œç”¨äº TensorRT-LLM
        self.spk_prompt_text_raw = {}
        
        # åˆå§‹åŒ–æ–‡æœ¬è§„èŒƒåŒ–å™¨ï¼ˆä½¿ç”¨ frontend çš„æˆç†Ÿå®ç°ï¼‰
        self.text_normalizer = None
        
        # å¼ºåˆ¶åˆå§‹åŒ– TensorRT-LLMï¼ˆè¿½æ±‚æè‡´æµå¼æ¨ç†æ€§èƒ½ï¼‰
        self._init_trtllm(trtllm_engine_dir, trtllm_tokenizer_dir)
        logging.info("âœ… TensorRT-LLM åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆå§‹åŒ–æ–‡æœ¬è§„èŒƒåŒ–å™¨ï¼ˆåœ¨ TensorRT-LLM tokenizer åˆå§‹åŒ–åï¼‰
        self.text_normalizer = TextNormalizer(
            tokenizer=self.trtllm_tokenizer,
            use_ttsfrd=USE_TTSFRD
        )
    
    def _init_trtllm(self, engine_dir, tokenizer_dir):
        """åˆå§‹åŒ– TensorRT-LLM å¼•æ“"""
        if not engine_dir or not os.path.exists(engine_dir):
            raise ValueError(f"TensorRT-LLM å¼•æ“ç›®å½•ä¸å­˜åœ¨: {engine_dir}")
        
        # è·å–å½“å‰è¿›ç¨‹çš„ MPI æ’å
        runtime_rank = tensorrt_llm.mpi_rank()
        # åˆå§‹åŒ– tokenizer
        self.trtllm_tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        
        # è®¾ç½®æ­£ç¡®çš„ chat template
        if 'system' in self.trtllm_tokenizer.chat_template:
            self.trtllm_tokenizer.chat_template = TRTLLM_CHAT_TEMPLATE
            logging.info("å·²è®¾ç½® CosyVoice2 ä¸“ç”¨ chat template")
        
        # EOS token ID
        self.eos_token_id = self.trtllm_tokenizer.convert_tokens_to_ids("<|eos1|>")
        
        # åˆå§‹åŒ– TensorRT-LLM ModelRunner
        runner_kwargs = dict(
            engine_dir=engine_dir,
            rank=runtime_rank,
            max_output_len=2048,    # æœ€å¤§è¾“å‡ºé•¿åº¦ï¼Œæ”¯æŒé•¿æ–‡æœ¬ç”Ÿæˆ
            enable_context_fmha_fp32_acc=False,
            max_batch_size=1,
            max_input_len=2048,     # ğŸ”´ å¢å¤§åˆ° 2048ï¼Œæ”¯æŒé•¿æ–‡æœ¬è¾“å…¥ï¼ˆprompt + tts_textï¼‰
            kv_cache_free_gpu_memory_fraction=0.25,  # é™ä½ KV ç¼“å­˜å ç”¨ï¼Œå¹³è¡¡æ˜¾å­˜
            cuda_graph_mode=False,
            gather_generation_logits=False,
        )
        self.trtllm_runner = ModelRunnerCpp.from_dir(**runner_kwargs)
        logging.info(f"TensorRT-LLM å¼•æ“å·²åŠ è½½: {engine_dir}")
    
    def _prepare_llm_input(self, tts_text, prompt_text, prompt_speech_tokens):
        """å‡†å¤‡ LLM è¾“å…¥ï¼ˆä½¿ç”¨ chat templateï¼‰
        
        Args:
            tts_text: è¦åˆæˆçš„ç›®æ ‡æ–‡æœ¬ï¼ˆåŸå§‹å­—ç¬¦ä¸²ï¼‰
            prompt_text: æç¤ºæ–‡æœ¬ï¼ˆåŸå§‹å­—ç¬¦ä¸²ï¼‰
            prompt_speech_tokens: æç¤ºè¯­éŸ³çš„ speech token IDsï¼ˆtensor æˆ– listï¼‰
        
        Returns:
            input_ids: tokenized åçš„è¾“å…¥ tensor
        """
        # 1. æ‹¼æ¥å®Œæ•´æ–‡æœ¬ï¼šprompt_text + tts_text
        full_text = prompt_text + tts_text
        
        # 2. å°† prompt_speech_tokens è½¬æ¢ä¸º <|s_XXXXX|> æ ¼å¼å­—ç¬¦ä¸²
        prompt_speech_str = convert_speech_tokens_to_str(prompt_speech_tokens)
        
        # 3. æ„å»º chat æ ¼å¼
        chat = [
            {"role": "user", "content": full_text},
            {"role": "assistant", "content": prompt_speech_str}
        ]
        
        # 4. ä½¿ç”¨ chat template è¿›è¡Œ tokenization
        input_ids = self.trtllm_tokenizer.apply_chat_template(
            chat,
            tokenize=True,
            return_tensors='pt',
            continue_final_message=True  # ç»§ç»­ç”Ÿæˆ assistant çš„å›å¤
        )
        
        return input_ids
    
    def _trtllm_generate_streaming(self, input_ids):
        """ä½¿ç”¨ TensorRT-LLM æµå¼ç”Ÿæˆ speech tokens
        
        Args:
            input_ids: tokenized è¾“å…¥ tensor
        
        Yields:
            Tuple[List[int], bool]: (å½“å‰ç´¯ç§¯çš„ speech_ids, æ˜¯å¦å®Œæˆ)
        """
        try:
            input_length = input_ids.shape[1]
            
            # TensorRT-LLM æµå¼ç”Ÿæˆ
            outputs_iter = self.trtllm_runner.generate(
                batch_input_ids=[input_ids[0]],
                max_new_tokens=2048,
                end_id=self.eos_token_id,
                pad_id=self.eos_token_id,
                temperature=0.8,
                top_k=50,
                top_p=0.95,
                repetition_penalty=1.1,
                streaming=True,  # å¯ç”¨æµå¼ç”Ÿæˆ
                output_sequence_lengths=True,
                return_dict=True,
            )
            
            # è¿­ä»£æµå¼è¾“å‡º
            for outputs in outputs_iter:
                torch.cuda.synchronize()
                
                # æå–ç”Ÿæˆçš„ token IDs
                output_ids = outputs["output_ids"]
                sequence_lengths = outputs["sequence_lengths"]
                
                # è·å–å®é™…ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆæ’é™¤è¾“å…¥ï¼‰
                actual_length = sequence_lengths[0][0].item()
                generated_ids = output_ids[0][0][input_length:actual_length].tolist()
                
                # å°† token IDs è§£ç ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åè§£æ <|s_XXXXX|> æ ¼å¼
                generated_tokens_str = self.trtllm_tokenizer.batch_decode(
                    [[tid] for tid in generated_ids],
                    skip_special_tokens=False
                )
                
                # æå–çœŸå®çš„ speech token IDs
                speech_ids = extract_speech_ids_from_str(generated_tokens_str)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªå“åº”
                is_final = outputs.get("finished", False)
                if isinstance(is_final, torch.Tensor):
                    is_final = is_final.item()
                
                yield speech_ids, is_final
                
                if is_final:
                    break
            
        except Exception as e:
            logging.error(f"TensorRT-LLM æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _trtllm_generate(self, input_ids):
        """ä½¿ç”¨ TensorRT-LLM ç”Ÿæˆ speech tokensï¼ˆéæµå¼ï¼Œç”¨äºå›é€€ï¼‰
        
        Args:
            input_ids: tokenized è¾“å…¥ tensor
        
        Returns:
            speech_ids: ç”Ÿæˆçš„ speech token IDs åˆ—è¡¨
        """
        try:
            input_length = input_ids.shape[1]
            
            # TensorRT-LLM ç”Ÿæˆ
            outputs = self.trtllm_runner.generate(
                batch_input_ids=[input_ids[0]],
                max_new_tokens=2048,
                end_id=self.eos_token_id,
                pad_id=self.eos_token_id,
                temperature=0.8,
                top_k=50,
                top_p=0.95,
                repetition_penalty=1.1,
                streaming=False,
                output_sequence_lengths=True,
                return_dict=True,
            )
            
            torch.cuda.synchronize()
            
            # æå–ç”Ÿæˆçš„ token IDs
            output_ids = outputs["output_ids"]
            sequence_lengths = outputs["sequence_lengths"]
            
            # è·å–å®é™…ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆæ’é™¤è¾“å…¥ï¼‰
            actual_length = sequence_lengths[0][0].item()
            generated_ids = output_ids[0][0][input_length:actual_length].tolist()
            
            # å°† token IDs è§£ç ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åè§£æ <|s_XXXXX|> æ ¼å¼
            generated_tokens_str = self.trtllm_tokenizer.batch_decode(
                [[tid] for tid in generated_ids],
                skip_special_tokens=False
            )
            
            # æå–çœŸå®çš„ speech token IDs
            speech_ids = extract_speech_ids_from_str(generated_tokens_str)
            
            logging.info(f"TensorRT-LLM ç”Ÿæˆäº† {len(speech_ids)} ä¸ª speech tokens")
            return speech_ids
            
        except Exception as e:
            logging.error(f"TensorRT-LLM ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def inference_zero_shot(self, text, prompt_text, prompt_speech_16k, zero_shot_spk_id='', stream=True, request_start_time=None):
        """é›¶æ ·æœ¬æ¨ç†ï¼ˆé›†æˆ TensorRT-LLM æµå¼ç”Ÿæˆ + é•¿æ–‡æœ¬åˆ†æ®µï¼‰"""
        # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´ï¼ˆå¦‚æœæœªä¼ å…¥ï¼‰
        if request_start_time is None:
            request_start_time = time.perf_counter()
        
        # ========== é•¿æ–‡æœ¬åˆ†æ®µé¢„å¤„ç† ==========
        # ä½¿ç”¨ frontend çš„æˆç†Ÿå®ç°ï¼šæ–‡æœ¬è§„èŒƒåŒ– + æ™ºèƒ½åˆ†æ®µ
        text_segments = self.text_normalizer.normalize_and_split(
            text, 
            token_max_n=80,  # ä¸ frontend.py ä¿æŒä¸€è‡´
            token_min_n=60
        )
        
        if len(text_segments) == 0:
            logging.warning("[æ–‡æœ¬åˆ†æ®µ] è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡æ¨ç†")
            return
        
        logging.info(f"[é•¿æ–‡æœ¬å¤„ç†] åŸå§‹æ–‡æœ¬ {len(text)} å­—ç¬¦ â†’ åˆ†æˆ {len(text_segments)} æ®µè¿›è¡Œæµå¼æ¨ç†")
        
        # ========== é€æ®µæ¨ç†å¹¶æµå¼è¿”å› ==========
        for segment_idx, text_segment in enumerate(text_segments):
            segment_start_time = time.perf_counter()
            logging.info(f"[æ®µè½æ¨ç† {segment_idx+1}/{len(text_segments)}] å¼€å§‹å¤„ç†: '{text_segment[:50]}{'...' if len(text_segment) > 50 else ''}'")
            
            # è°ƒç”¨å•æ®µæ¨ç†ï¼ˆå†…éƒ¨é€»è¾‘ä¿æŒä¸å˜ï¼‰
            for output in self._inference_single_segment(
                text_segment, prompt_text, prompt_speech_16k,
                zero_shot_spk_id=zero_shot_spk_id,
                stream=stream,
                request_start_time=request_start_time if segment_idx == 0 else segment_start_time,
                is_first_segment=(segment_idx == 0)
            ):
                yield output
            
            segment_time = (time.perf_counter() - segment_start_time) * 1000
            logging.info(f"[æ®µè½æ¨ç† {segment_idx+1}/{len(text_segments)}] å®Œæˆï¼Œè€—æ—¶: {segment_time:.2f}ms")
    
    def _inference_single_segment(self, text, prompt_text, prompt_speech_16k, zero_shot_spk_id='', stream=True, request_start_time=None, is_first_segment=True):
        """å•æ®µæ–‡æœ¬æ¨ç†ï¼ˆTensorRT-LLM æµå¼ç”Ÿæˆ + æµå¼ token2wavï¼ŒåŸ inference_zero_shot çš„æ ¸å¿ƒé€»è¾‘ï¼‰"""
        try:
            # ========== é˜¶æ®µ 1: ä¸Šä¸‹æ–‡åŠ è½½ ==========
            context_start = time.perf_counter()
            
            # 1. è·å– speaker infoï¼ˆåŒ…å« prompt_speech_tokensï¼‰
            spk_info = self.cosyvoice.frontend.spk2info.get(zero_shot_spk_id)
            if spk_info is None:
                raise ValueError(f"Speaker {zero_shot_spk_id} ä¸å­˜åœ¨")
            
            # 2. è·å–åŸå§‹çš„ prompt_text å­—ç¬¦ä¸²
            prompt_text_raw = self.spk_prompt_text_raw.get(zero_shot_spk_id, '')
            if not prompt_text_raw:
                logging.warning(f"Speaker {zero_shot_spk_id} ç¼ºå°‘åŸå§‹ prompt_textï¼Œå›é€€åˆ° PyTorch æ¨ç†")
                raise ValueError("ç¼ºå°‘åŸå§‹ prompt_text")
            
            # 3. è·å– spk_info ä¸­çš„æ•°æ®
            llm_prompt_speech_token = spk_info['llm_prompt_speech_token']
            flow_prompt_speech_token = spk_info['flow_prompt_speech_token']
            prompt_speech_feat = spk_info['prompt_speech_feat']
            flow_embedding = spk_info['flow_embedding']
            
            context_load_time = (time.perf_counter() - context_start) * 1000
            if is_first_segment:
                logging.info(f"[å»¶è¿Ÿåˆ†æ-01] ä¸Šä¸‹æ–‡åŠ è½½: {context_load_time:.2f}ms (spk_infoæ£€ç´¢+æ•°æ®è§£æ)")
            
            # ========== é˜¶æ®µ 2: LLM è¾“å…¥å‡†å¤‡ ==========
            prepare_start = time.perf_counter()
            
            # 4. å‡†å¤‡ LLM è¾“å…¥ï¼ˆä½¿ç”¨åŸå§‹å­—ç¬¦ä¸² + chat templateï¼‰
            input_ids = self._prepare_llm_input(text, prompt_text_raw, llm_prompt_speech_token)
            
            prepare_time = (time.perf_counter() - prepare_start) * 1000
            if is_first_segment:
                logging.info(f"[å»¶è¿Ÿåˆ†æ-02] LLMè¾“å…¥å‡†å¤‡: {prepare_time:.2f}ms (text:{len(text)} chars, prompt:{len(prompt_text_raw)} chars, input_tokens:{input_ids.shape[1]})")
            
            # ========== é˜¶æ®µ 3: æ¨ç†å‚æ•°åˆå§‹åŒ– ==========
            init_start = time.perf_counter()
            
            # 5. åˆå§‹åŒ–æµå¼å‚æ•°
            this_uuid = str(uuid_module.uuid1())
            model = self.cosyvoice.model
            model.hift_cache_dict[this_uuid] = None
            
            # æ ¸å¿ƒå‚æ•°ï¼šä¿æŒåŸå§‹å¯¹é½é€»è¾‘ä¸å˜
            token_hop_len = 25  # æ ‡å‡† hop é•¿åº¦ï¼ˆä¿æŒåŸå§‹è®¾ç½®ï¼‰
            token_hop_len_first = 15  # é¦–å—ä½¿ç”¨æ›´å°çš„ hopï¼Œå‡å°‘ç­‰å¾…
            pre_lookahead_len = model.flow.pre_lookahead_len  # å‰ç»é•¿åº¦ (3)
            
            # å…³é”®ï¼šprompt_token_pad å¿…é¡»åŸºäºæ ‡å‡† hop_len è®¡ç®—ï¼Œä¸èƒ½æ”¹å˜
            prompt_token_pad = int(np.ceil(flow_prompt_speech_token.shape[1] / token_hop_len) * token_hop_len - flow_prompt_speech_token.shape[1])
            
            token_offset = 0
            chunk_idx = 0
            
            # RTF ç»Ÿè®¡
            total_audio_duration = 0.0  # ç´¯è®¡ç”Ÿæˆçš„éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
            total_processing_time = 0.0  # ç´¯è®¡å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
            sample_rate = 22050  # CosyVoice2 çš„é‡‡æ ·ç‡
            
            init_time = (time.perf_counter() - init_start) * 1000
            # é¦–å—å®é™…éœ€è¦çš„ tokens æ•°é‡
            first_chunk_tokens_needed = token_hop_len_first + prompt_token_pad + pre_lookahead_len
            if is_first_segment:
                logging.info(f"[å»¶è¿Ÿåˆ†æ-03] æ¨ç†å‚æ•°åˆå§‹åŒ–: {init_time:.2f}ms (hop_first={token_hop_len_first}, hop_normal={token_hop_len}, prompt_pad={prompt_token_pad}, lookahead={pre_lookahead_len}, first_needed={first_chunk_tokens_needed})")
                logging.info(f"[æµå¼ç”Ÿæˆ] å¼€å§‹æµå¼ç”Ÿæˆ+token2wav: first_chunk_needed={first_chunk_tokens_needed} tokens (åŸå§‹é…ç½®éœ€28)")
            
            # ========== é˜¶æ®µ 4: Token ç”Ÿæˆ ==========
            token_gen_start = time.perf_counter()
            first_token_gen_time = None
            
            # 6. æµå¼ç”Ÿæˆ + æµå¼ token2wav
            speech_tokens = []
            generation_done = False
            first_chunk_generated = False
            
            for current_tokens, is_final in self._trtllm_generate_streaming(input_ids):
                # è®°å½•é¦–ä¸ª token ç”Ÿæˆå®Œæ¯•çš„æ—¶é—´
                if first_token_gen_time is None:
                    first_token_gen_time = (time.perf_counter() - token_gen_start) * 1000
                    if is_first_segment:
                        logging.info(f"[å»¶è¿Ÿåˆ†æ-04a] é¦–ä¸ªTokenç”Ÿæˆå®Œæ¯•: {first_token_gen_time:.2f}ms (é¦–åŒ…tokens: {len(current_tokens)})")
                
                speech_tokens = current_tokens
                generation_done = is_final
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ tokens ç”Ÿæˆç¬¬ä¸€å—éŸ³é¢‘
                while True:
                    # é¦–å—ä½¿ç”¨å° hopï¼Œåç»­å—ä½¿ç”¨æ­£å¸¸hop
                    if token_offset == 0:
                        this_token_hop_len = token_hop_len_first + prompt_token_pad
                    else:
                        this_token_hop_len = token_hop_len
                    
                    tokens_needed = token_offset + this_token_hop_len + pre_lookahead_len
                    
                    if tokens_needed <= len(speech_tokens):
                        if not first_chunk_generated:
                            # ========== é˜¶æ®µ 5: é¦–å—éŸ³é¢‘åˆæˆ ==========
                            first_chunk_start = time.perf_counter()
                            accumulated_tokens_time = (first_chunk_start - token_gen_start) * 1000
                            if is_first_segment:
                                logging.info(f"[å»¶è¿Ÿåˆ†æ-04b] Tokenç´¯ç§¯åˆ°é¦–å—éœ€é‡: {accumulated_tokens_time:.2f}ms (å·²ç´¯ç§¯tokens: {len(speech_tokens)}/{tokens_needed})")
                        
                        # æœ‰è¶³å¤Ÿçš„ tokensï¼Œç”Ÿæˆä¸€å—éŸ³é¢‘
                        chunk_start_time = time.perf_counter()
                        
                        this_tts_speech_token = torch.tensor(
                            speech_tokens[:tokens_needed]
                        ).unsqueeze(0)
                        
                        tts_speech = model.token2wav(
                            token=this_tts_speech_token,
                            prompt_token=flow_prompt_speech_token,
                            prompt_feat=prompt_speech_feat,
                            embedding=flow_embedding,
                            token_offset=token_offset,
                            uuid=this_uuid,
                            stream=True,
                            finalize=False,
                            speed=1.0
                        )
                        
                        chunk_time = (time.perf_counter() - chunk_start_time) * 1000
                        
                        # è®¡ç®—å½“å‰å—çš„éŸ³é¢‘æ—¶é•¿å’Œ RTF
                        chunk_audio_duration = tts_speech.shape[-1] / sample_rate  # ç§’
                        chunk_rtf = (chunk_time / 1000) / chunk_audio_duration if chunk_audio_duration > 0 else 0
                        total_audio_duration += chunk_audio_duration
                        total_processing_time += (chunk_time / 1000)
                        cumulative_rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0
                        
                        if not first_chunk_generated:
                            if is_first_segment:
                                logging.info(f"[å»¶è¿Ÿåˆ†æ-05] é¦–å—éŸ³é¢‘åˆæˆ(token2wav): {chunk_time:.2f}ms (tokens: {tokens_needed}, hop: {this_token_hop_len})")
                                logging.info(f"[é¦–å—RTF] éŸ³é¢‘æ—¶é•¿: {chunk_audio_duration*1000:.1f}ms, å¤„ç†è€—æ—¶: {chunk_time:.1f}ms, RTF: {chunk_rtf:.3f}")
                            first_chunk_generated = True
                            
                            # è®¡ç®—ä»è¯·æ±‚åˆ°é¦–åŒ…çš„æ€»å»¶è¿Ÿï¼ˆä»…é¦–æ®µï¼‰
                            if is_first_segment:
                                total_ttfb = (time.perf_counter() - request_start_time) * 1000
                                logging.info(f"\n{'='*70}")
                                logging.info(f"[é¦–åŒ…å»¶è¿Ÿæ±‡æ€» TTFB] æ€»è€—æ—¶: {total_ttfb:.2f}ms")
                                logging.info(f"  â”œâ”€ ä¸Šä¸‹æ–‡åŠ è½½: {context_load_time:.2f}ms (step 1)")
                                logging.info(f"  â”œâ”€ LLMè¾“å…¥å‡†å¤‡: {prepare_time:.2f}ms (step 2)")
                                logging.info(f"  â”œâ”€ å‚æ•°åˆå§‹åŒ–: {init_time:.2f}ms (step 3)")
                                logging.info(f"  â”œâ”€ Tokenç”Ÿæˆ(é¦–ä¸ª): {first_token_gen_time:.2f}ms (step 4a)")
                                logging.info(f"  â”œâ”€ Tokenç´¯ç§¯ç­‰å¾…: {accumulated_tokens_time - first_token_gen_time:.2f}ms (step 4b)")
                                logging.info(f"  â””â”€ éŸ³é¢‘åˆæˆ(token2wav): {chunk_time:.2f}ms (step 5)")
                                logging.info(f"[å»¶è¿Ÿåˆ†è§£] Model:{context_load_time+prepare_time+init_time:.1f}ms + LLMGen:{first_token_gen_time:.1f}ms + TTW:{chunk_time:.1f}ms + Wait:{accumulated_tokens_time - first_token_gen_time:.1f}ms = {total_ttfb:.1f}ms")
                                logging.info(f"[æ€§èƒ½æŒ‡æ ‡] é¦–å—RTF: {chunk_rtf:.3f}, éŸ³é¢‘: {chunk_audio_duration*1000:.0f}ms, ç›®æ ‡RTF: <0.2")
                                logging.info(f"{'='*70}\n")
                        else:
                            logging.info(f"[æµå¼token2wav] å—{chunk_idx}: éŸ³é¢‘={chunk_audio_duration*1000:.0f}ms, è€—æ—¶={chunk_time:.1f}ms, RTF={chunk_rtf:.3f}, ç´¯ç§¯RTF={cumulative_rtf:.3f}")
                        
                        token_offset += this_token_hop_len
                        chunk_idx += 1
                        yield {'tts_speech': tts_speech.cpu()}
                    else:
                        # ä¸å¤Ÿ tokensï¼Œç­‰å¾…æ›´å¤šç”Ÿæˆ
                        break
                
                if generation_done:
                    break
            
            # ========== é˜¶æ®µ 6: å¤„ç†å‰©ä½™Tokens ==========
            # 7. å¤„ç†å‰©ä½™çš„ tokensï¼ˆæœ€åä¸€å—ï¼‰
            if token_offset < len(speech_tokens):
                chunk_start_time = time.perf_counter()
                
                this_tts_speech_token = torch.tensor(speech_tokens).unsqueeze(0)
                
                tts_speech = model.token2wav(
                    token=this_tts_speech_token,
                    prompt_token=flow_prompt_speech_token,
                    prompt_feat=prompt_speech_feat,
                    embedding=flow_embedding,
                    token_offset=token_offset,
                    uuid=this_uuid,
                    stream=True,
                    finalize=True,
                    speed=1.0
                )
                
                chunk_time = (time.perf_counter() - chunk_start_time) * 1000
                
                # è®¡ç®—æœ€ç»ˆå—çš„éŸ³é¢‘æ—¶é•¿å’Œ RTF
                chunk_audio_duration = tts_speech.shape[-1] / sample_rate
                chunk_rtf = (chunk_time / 1000) / chunk_audio_duration if chunk_audio_duration > 0 else 0
                total_audio_duration += chunk_audio_duration
                total_processing_time += (chunk_time / 1000)
                cumulative_rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0
                
                logging.info(f"[æµå¼token2wav] æœ€ç»ˆå—{chunk_idx}: éŸ³é¢‘={chunk_audio_duration*1000:.0f}ms, è€—æ—¶={chunk_time:.1f}ms, RTF={chunk_rtf:.3f} (finalize)")
                yield {'tts_speech': tts_speech.cpu()}  # ğŸ”´ å…³é”®ï¼šå¿…é¡» yield æœ€åä¸€å—éŸ³é¢‘
            
            # è¾“å‡ºæ€»ä½“ RTF ç»Ÿè®¡ï¼ˆä»…é¦–æ®µè¯¦ç»†è¾“å‡ºï¼‰
            overall_rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0
            if is_first_segment:
                logging.info(f"[FastTTS] æµå¼ç”Ÿæˆå®Œæˆ: å…± {len(speech_tokens)} ä¸ª speech tokens, {chunk_idx + 1} ä¸ªéŸ³é¢‘å—")
                logging.info(f"[æ•´ä½“RTFç»Ÿè®¡] æ€»éŸ³é¢‘æ—¶é•¿: {total_audio_duration:.2f}s, æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f}s, æ•´ä½“RTF: {overall_rtf:.3f} (ç›®æ ‡: <0.2)")
            
            # æ¸…ç†ç¼“å­˜
            if this_uuid in model.hift_cache_dict:
                model.hift_cache_dict.pop(this_uuid)
            
        except Exception as e:
            logging.error(f"TensorRT-LLM æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸å›é€€åˆ° PyTorch
    
    def list_available_spks(self):
        """è·å–å¯ç”¨è¯´è¯äººåˆ—è¡¨"""
        return self.cosyvoice.list_available_spks()
    
    def add_zero_shot_spk(self, prompt_text, prompt_speech_16k, zero_shot_spk_id):
        """æ·»åŠ é›¶æ ·æœ¬è¯´è¯äºº"""
        # ä¿å­˜åŸå§‹çš„ prompt_text å­—ç¬¦ä¸²ï¼ˆç”¨äº TensorRT-LLMï¼‰
        self.spk_prompt_text_raw[zero_shot_spk_id] = prompt_text
        return self.cosyvoice.add_zero_shot_spk(prompt_text, prompt_speech_16k, zero_shot_spk_id)
    
    def save_spkinfo(self):
        """ä¿å­˜è¯´è¯äººä¿¡æ¯åˆ°æŒ‡å®šè·¯å¾„"""
        if self.spk2info_path:
            torch.save(self.cosyvoice.frontend.spk2info, self.spk2info_path)
            logging.info(f"è¯´è¯äººä¿¡æ¯å·²ä¿å­˜åˆ°: {self.spk2info_path}")
        else:
            return self.cosyvoice.save_spkinfo()
    
    def load_spk_prompt_text_raw(self, spk_prompt_text_raw_dict):
        """ä»å¤–éƒ¨åŠ è½½åŸå§‹ prompt_text æ˜ å°„"""
        self.spk_prompt_text_raw.update(spk_prompt_text_raw_dict)


def generate_data(model_output, request_start_time):
    """ç”ŸæˆéŸ³é¢‘æ•°æ®æµï¼Œå¯¹è¾“å‡ºè¿›è¡Œå‰Šæ³¢å¤„ç†é˜²æ­¢çˆ†éŸ³"""
    is_first = True
    chunk_count = 0
    
    for i in model_output:
        if is_first:
            first_chunk_time = time.perf_counter()
            ttfb = (first_chunk_time - request_start_time) * 1000
            # æ³¨æ„ï¼šæ­¤å¤„ TTFB æ˜¯ HTTP å“åº”çº§åˆ«çš„é¦–åŒ…æ—¶é—´ï¼ˆä»è¯·æ±‚åˆ° generate_data é¦–æ¬¡äº§å‡ºæ•°æ®ï¼‰
            # å®é™…çš„æ¨ç† TTFB å·²åœ¨ inference_zero_shot ä¸­è¯¦ç»†æ‰“å°
            logging.info(f"[TTSç»Ÿè®¡] HTTPå“åº”é¦–åŒ…ç”Ÿæˆå®Œæ¯•! HTTP TTFB: {ttfb:.2f}ms")
            is_first = False

        tts_speech = i["tts_speech"].numpy()
        
        # è¾“å‡ºç«¯å‰Šæ³¢ï¼šé˜²æ­¢ float -> int16 è½¬æ¢æ—¶çš„æ•´æ•°æº¢å‡º
        tts_speech = np.clip(tts_speech, -1.0, 1.0)
        
        # è½¬æ¢ä¸º int16 æ ¼å¼
        tts_audio = (tts_speech * 32767.0).astype(np.int16).tobytes()
        chunk_count += 1
        yield tts_audio
    
    total_time = (time.perf_counter() - request_start_time) * 1000
    logging.info(f"[TTSç»Ÿè®¡] æµå¼ä¼ è¾“ç»“æŸ. æ€»è€—æ—¶: {total_time:.2f}ms, å…±å‘é€ {chunk_count} ä¸ªæ•°æ®å—")


def load_speakers_from_directory(speaker_dir="asset/speakers"):
    """ä»ç›®å½•åŠ è½½æ‰€æœ‰è¯´è¯äºº"""
    speakers = {}
    
    if not os.path.exists(speaker_dir):
        logging.warning(f"è¯´è¯äººç›®å½• {speaker_dir} ä¸å­˜åœ¨")
        return speakers
    
    wav_files = glob.glob(os.path.join(speaker_dir, "*.wav"))
    
    for wav_path in wav_files:
        filename = os.path.basename(wav_path)
        # è§£ææ–‡ä»¶åæ ¼å¼ï¼š[è¯´è¯äººåç§°]æ–‡æœ¬å†…å®¹.wav
        match = re.match(r'\[(.+?)\](.+)\.wav$', filename)
        
        if match:
            speaker_name = match.group(1)
            prompt_text = match.group(2)
            
            try:
                prompt_speech_16k = load_wav(wav_path, 16000)
                
                # è¾“å…¥ç«¯å½’ä¸€åŒ–
                max_val = torch.abs(prompt_speech_16k).max().item()
                target_peak = 0.95
                
                if max_val > target_peak:
                    logging.warning(f"è¯´è¯äºº {speaker_name} éŸ³é¢‘å³°å€¼ {max_val:.4f} è¶…å‡ºå®‰å…¨èŒƒå›´ï¼Œå½’ä¸€åŒ–åˆ° {target_peak}")
                    prompt_speech_16k = (prompt_speech_16k / max_val) * target_peak
                
                speakers[speaker_name] = {
                    'prompt_text': prompt_text,
                    'prompt_speech_16k': prompt_speech_16k,
                    'wav_path': wav_path
                }
                logging.info(f"åŠ è½½è¯´è¯äºº: {speaker_name}")
            except Exception as e:
                logging.error(f"åŠ è½½è¯´è¯äºº {speaker_name} å¤±è´¥: {e}")
        else:
            logging.warning(f"æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡: {filename}")
    
    return speakers


def extract_spk_prompt_text_from_directory(speaker_dir="asset/speakers"):
    """ä»è¯´è¯äººç›®å½•æå– spk_id -> prompt_text æ˜ å°„ï¼ˆä¸åŠ è½½éŸ³é¢‘ï¼‰"""
    spk_prompt_text_raw = {}
    
    if not os.path.exists(speaker_dir):
        logging.warning(f"è¯´è¯äººç›®å½• {speaker_dir} ä¸å­˜åœ¨")
        return spk_prompt_text_raw
    
    wav_files = glob.glob(os.path.join(speaker_dir, "*.wav"))
    
    for wav_path in wav_files:
        filename = os.path.basename(wav_path)
        # è§£ææ–‡ä»¶åæ ¼å¼ï¼š[è¯´è¯äººåç§°]æ–‡æœ¬å†…å®¹.wav
        match = re.match(r'\[(.+?)\](.+)\.wav$', filename)
        
        if match:
            speaker_name = match.group(1)
            prompt_text = match.group(2)
            spk_prompt_text_raw[speaker_name] = prompt_text
    
    return spk_prompt_text_raw


@app.get("/")
async def index():
    """ä¸»é¡µè·¯ç”±ï¼Œè¿”å›å‰ç«¯é¡µé¢"""
    index_path = os.path.join(os.path.dirname(__file__), "static", "index.html")
    if os.path.exists(index_path):
        return FileResponse(index_path)
    return {"message": "FastCosyVoice TTS Server (TensorRT-LLM Accelerated) is running. Visit /static/index.html for the web interface."}


@app.get("/api/speakers")
async def get_speakers():
    """è·å–æ‰€æœ‰å¯ç”¨çš„è¯´è¯äººåˆ—è¡¨"""
    try:
        speakers = fast_cosyvoice.list_available_spks()
        return JSONResponse(content={"speakers": speakers})
    except Exception as e:
        logging.error(f"è·å–è¯´è¯äººåˆ—è¡¨å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/tts")
async def inference_zero_shot(request: Request, text: str = Form(), speaker: str = Form(default="")):
    """æ–‡æœ¬è½¬è¯­éŸ³æ¥å£ï¼ˆé›†æˆ TensorRT-LLM åŠ é€Ÿï¼‰"""
    # ä½¿ç”¨ä¸­é—´ä»¶è®°å½•çš„å¼€å§‹æ—¶é—´ï¼Œç¡®ä¿ä¸å‰ç«¯å¯¹é½
    request_start_time = request.state.start_time
    logging.info(f"[FastTTSè¯·æ±‚] æ”¶åˆ°è¯·æ±‚: text='{text[:50] if len(text) > 50 else text}', speaker='{speaker}'")

    try:
        # è·å–å¯ç”¨è¯´è¯äººåˆ—è¡¨
        available_spks = fast_cosyvoice.list_available_spks()
        # é»˜è®¤ä½¿ç”¨jokè€å¸ˆï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªè¯´è¯äºº
        default_speaker = "jokè€å¸ˆ" if "jokè€å¸ˆ" in available_spks else (available_spks[0] if available_spks else "")
        selected_speaker = speaker if speaker else default_speaker
        
        if not selected_speaker:
            return JSONResponse(content={"error": "æ²¡æœ‰å¯ç”¨çš„è¯´è¯äºº"}, status_code=400)
        
        logging.info(f"[FastTTSæ¨ç†] å¼€å§‹æ¨ç†, è¯´è¯äºº: {selected_speaker}")

        # ä½¿ç”¨ FastCosyVoice2 è¿›è¡Œæ¨ç†ï¼ˆè‡ªåŠ¨é€‰æ‹© TensorRT-LLM æˆ– PyTorchï¼‰
        model_output = fast_cosyvoice.inference_zero_shot(
            text, "", None, 
            zero_shot_spk_id=selected_speaker,
            stream=True,
            request_start_time=request_start_time
        )
        return StreamingResponse(generate_data(model_output, request_start_time))
    except Exception as e:
        logging.error(f"FastTTSæ¨ç†å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=50000, help="æœåŠ¡ç«¯å£")
    parser.add_argument(
        "--model_dir",
        type=str,
        default="pretrained_models/CosyVoice2-0.5B",
        help="æ¨¡å‹æœ¬åœ°è·¯å¾„æˆ– modelscope ä»“åº“ id",
    )
    parser.add_argument(
        "--trtllm_engine_dir",
        type=str,
        default="pretrained_models/cosyvoice2_llm/trt_engines_bfloat16",
        help="TensorRT-LLM å¼•æ“ç›®å½•",
    )
    parser.add_argument(
        "--trtllm_tokenizer_dir",
        type=str,
        default="pretrained_models/cosyvoice2_llm",
        help="TensorRT-LLM tokenizer ç›®å½•",
    )
    parser.add_argument(
        "--speaker_dir",
        type=str,
        default="asset/speakers",
        help="è¯´è¯äººéŸ³é¢‘æ–‡ä»¶ç›®å½•",
    )
    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–åŸºç¡€ CosyVoice2 æ¨¡å‹
        logging.info("=" * 60)
        logging.info("å¯åŠ¨ FastCosyVoice TTS Server")
        logging.info(f"æ¨¡å‹ç›®å½•: {args.model_dir}")
        logging.info(f"TensorRT-LLM å¼•æ“ç›®å½•: {args.trtllm_engine_dir}")
        logging.info(f"TensorRT-LLM Tokenizer: {args.trtllm_tokenizer_dir}")
        logging.info("=" * 60)
        
        # åŠ è½½åŸå§‹ CosyVoice2 æ¨¡å‹ï¼ˆç¡®ä¿å¯ç”¨æ‰€æœ‰åŠ é€Ÿé€‰é¡¹ï¼‰
        cosyvoice = CosyVoice2(
            args.model_dir, 
            load_jit=True,   # âœ… JITç¼–è¯‘åŠ é€Ÿ
            load_trt=True,   # âœ… TensorRTä¼˜åŒ–
            fp16=True        # âœ… FP16æ··åˆç²¾åº¦
        )
        logging.info("âœ… æ¨¡å‹åŠ è½½é…ç½®: JIT=True, TRT=True, FP16=True")
        
        # åˆ›å»º FastCosyVoice2 å®ä¾‹ï¼ˆé›†æˆ TensorRT-LLMï¼‰
        spk2info_path = os.path.join(args.speaker_dir, 'spk2info.pt')
        fast_cosyvoice = FastCosyVoice2(
            cosyvoice_model=cosyvoice,
            trtllm_engine_dir=args.trtllm_engine_dir,
            trtllm_tokenizer_dir=args.trtllm_tokenizer_dir,
            spk2info_path=spk2info_path,
        )
        try:
            # æŠŠ PyTorch LLM ç§»åˆ° CPUï¼Œé‡Šæ”¾ GPU æ˜¾å­˜
            fast_cosyvoice.cosyvoice.model.llm.to("cpu")
            torch.cuda.empty_cache()
            logging.info("å·²å°† CosyVoice2 PyTorch LLM ç§»è‡³ CPUï¼Œé‡Šæ”¾ GPU æ˜¾å­˜")
        except Exception as e:
            logging.warning(f"ç§»åŠ¨ CosyVoice2 LLM åˆ° CPU å¤±è´¥: {e}")
        logging.info("âœ… FastCosyVoice2 åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ TensorRT-LLM åŠ é€Ÿ")

    except Exception as e:
        raise TypeError(f"å¯¼å…¥{args.model_dir}å¤±è´¥ï¼Œæ¨¡å‹ç±»å‹æœ‰è¯¯ï¼é”™è¯¯: {e}")

    # åŠ è½½è¯´è¯äººä¿¡æ¯
    spk2info_path = os.path.join(args.speaker_dir, 'spk2info.pt')
    spk_prompt_text_raw_map = extract_spk_prompt_text_from_directory(args.speaker_dir)
    
    # æ£€æŸ¥ speakers ç›®å½•ä¸‹çš„ spk2info.pt æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ‰€æœ‰è¯´è¯äºº
    need_regenerate = False
    
    # å¦‚æœ speakers ç›®å½•ä¸‹æœ‰ spk2info.ptï¼ŒåŠ è½½å®ƒ
    if os.path.exists(spk2info_path):
        spk2info_data = torch.load(spk2info_path, map_location=fast_cosyvoice.device)
        fast_cosyvoice.cosyvoice.frontend.spk2info.update(spk2info_data)
        logging.info(f"å·²åŠ è½½ spk2info.pt: {spk2info_path}")
    
    existing_spks = set(fast_cosyvoice.cosyvoice.frontend.spk2info.keys())
    required_spks = set(spk_prompt_text_raw_map.keys())
    
    if not os.path.exists(spk2info_path):
        print(f"æœªæ‰¾åˆ° spk2info.ptï¼Œå°†ç”Ÿæˆæ–°æ–‡ä»¶")
        need_regenerate = True
    elif required_spks - existing_spks:
        missing_spks = required_spks - existing_spks
        print(f"æ£€æµ‹åˆ°æ–°å¢è¯´è¯äºº: {missing_spks}ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ spk2info.pt")
        need_regenerate = True
    else:
        print(f"spk2info.pt å·²å­˜åœ¨ï¼ŒåŒ…å« {len(existing_spks)} ä¸ªè¯´è¯äººï¼Œè·³è¿‡ç‰¹å¾æå–")
    
    if need_regenerate:
        print("æ­£åœ¨æå–è¯´è¯äººéŸ³é¢‘ç‰¹å¾...")
        speakers_data = load_speakers_from_directory(args.speaker_dir)
        
        if not speakers_data:
            print(f"è­¦å‘Šï¼šæœªåœ¨ {args.speaker_dir} ç›®å½•æ‰¾åˆ°ä»»ä½•è¯´è¯äººæ–‡ä»¶")
        else:
            print(f"æˆåŠŸåŠ è½½ {len(speakers_data)} ä¸ªè¯´è¯äºº")
            
            # å°†æ‰€æœ‰è¯´è¯äººæ·»åŠ åˆ°æ¨¡å‹
            for speaker_name, speaker_info in speakers_data.items():
                try:
                    fast_cosyvoice.add_zero_shot_spk(
                        speaker_info['prompt_text'],
                        speaker_info['prompt_speech_16k'],
                        speaker_name
                    )
                    print(f"  âœ“ {speaker_name}")
                except Exception as e:
                    print(f"  âœ— {speaker_name}: {e}")
            
            # ä¿å­˜è¯´è¯äººä¿¡æ¯
            try:
                # ç¡®ä¿ speaker_dir å­˜åœ¨
                os.makedirs(args.speaker_dir, exist_ok=True)
                fast_cosyvoice.save_spkinfo()
                print(f"è¯´è¯äººä¿¡æ¯å·²ä¿å­˜åˆ° {spk2info_path}")
            except Exception as e:
                print(f"ä¿å­˜è¯´è¯äººä¿¡æ¯å¤±è´¥: {e}")
    else:
        # ç›´æ¥ä»æ–‡ä»¶ååŠ è½½ prompt_text æ˜ å°„
        fast_cosyvoice.load_spk_prompt_text_raw(spk_prompt_text_raw_map)
        print(f"å·²ä»æ–‡ä»¶ååŠ è½½ {len(spk_prompt_text_raw_map)} ä¸ªè¯´è¯äººçš„ prompt_text æ˜ å°„")
    
    # æ¨¡å‹é¢„çƒ­
    print("\næ­£åœ¨é¢„çƒ­æ¨¡å‹...")
    available_spks = fast_cosyvoice.list_available_spks()
    if available_spks:
        warmup_speaker = "jokè€å¸ˆ" if "jokè€å¸ˆ" in available_spks else available_spks[0]
        print(f"ä½¿ç”¨ '{warmup_speaker}' è¿›è¡Œé¢„çƒ­")
        warmup_texts = [
            "ä½ å¥½ã€‚",  # çŸ­å¥
            "è¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„çƒ­æ¨¡å‹çš„æµ‹è¯•å¥å­ï¼Œç¡®ä¿æœåŠ¡å“åº”é€Ÿåº¦ã€‚", # ä¸­å¥
            "è¯­éŸ³åˆæˆæœåŠ¡æ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨å€™ï¼Œç³»ç»Ÿæ­£åœ¨è¿›è¡Œåˆå§‹åŒ–æ“ä½œã€‚", # é•¿å¥
            "å¥½çš„ï¼Œæ²¡é—®é¢˜ã€‚" # çŸ­å¥
        ]
        
        for t in warmup_texts:
            try:
                for _ in fast_cosyvoice.inference_zero_shot(
                        t, "", None, zero_shot_spk_id=warmup_speaker, stream=True):
                    pass
            except Exception as e:
                print(f"é¢„çƒ­å¤±è´¥: {e}")
                break
    else:
        print("æœªæ‰¾åˆ°å¯ç”¨è¯´è¯äººï¼Œè·³è¿‡é¢„çƒ­")
    
    print("é¢„çƒ­å®Œæ¯•\n")
    print("=" * 60)
    print(f"ğŸš€ FastCosyVoice TTS Server å¯åŠ¨åœ¨ç«¯å£ {args.port}")

    # é…ç½® uvicorn ä»¥æ”¯æŒ HTTP keep-alive
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=args.port,
        timeout_keep_alive=60,  # keep-alive è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        limit_concurrency=100,  # æœ€å¤§å¹¶å‘è¿æ¥æ•°
        backlog=2048,  # TCP backlog é˜Ÿåˆ—å¤§å°
    )
