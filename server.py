import os
import time
import sys
import argparse
import logging
import re
import glob
import json
from functools import partial
import inflect

# é…ç½®æ—¥å¿—æ ¼å¼ï¼Œç¡®ä¿æ˜¾ç¤ºæ¯«ç§’çº§æ—¶é—´æˆ³
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s.%(msecs)03d %(levelname)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logging.getLogger("matplotlib").setLevel(logging.WARNING)

from fastapi import FastAPI, Form, Request
from fastapi.responses import StreamingResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
import uvicorn
import numpy as np
import torch

# æ·»åŠ  Matcha-TTS è·¯å¾„
sys.path.append("third_party/Matcha-TTS")

from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.file_utils import load_wav
from cosyvoice.utils.frontend_utils import (
    contains_chinese, replace_blank, replace_corner_mark, 
    remove_bracket, spell_out_number, split_paragraph, is_only_punctuation
)
import uuid as uuid_module

# å¯¼å…¥ ttsfrd æ¨¡å—ï¼ˆç”¨äºæ–‡æœ¬è§„èŒƒåŒ–ï¼‰
try:
    import ttsfrd
    USE_TTSFRD = True
    logging.info("å·²å¯¼å…¥ ttsfrd æ¨¡å—ç”¨äºæ–‡æœ¬è§„èŒƒåŒ–")
except ImportError:
    USE_TTSFRD = False
    logging.warning("ttsfrd ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ wetext è¿›è¡Œæ–‡æœ¬è§„èŒƒåŒ–")
    try:
        from wetext import Normalizer as ZhNormalizer
        from wetext import Normalizer as EnNormalizer
    except ImportError:
        logging.warning("wetext ä¹Ÿä¸å¯ç”¨ï¼Œæ–‡æœ¬è§„èŒƒåŒ–åŠŸèƒ½å°†å—é™")

app = FastAPI()

# æ·»åŠ è¯·æ±‚è®¡æ—¶ä¸­é—´ä»¶ï¼ˆå¿…é¡»åœ¨ CORS ä¹‹å‰ï¼Œç¡®ä¿æœ€æ—©å¼€å§‹è®¡æ—¶ï¼‰
class TimingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # åœ¨è¯·æ±‚åˆšåˆ°è¾¾æ—¶ç«‹å³å¼€å§‹è®¡æ—¶
        request.state.start_time = time.perf_counter()
        response = await call_next(request)
        return response

app.add_middleware(TimingMiddleware)

# è®¾ç½®åŒæºç­–ç•¥
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")


class TextNormalizer:
    """æ–‡æœ¬è§„èŒƒåŒ–å·¥å…·ç±»ï¼Œå¤ç”¨ CosyVoice frontend çš„æˆç†Ÿå®ç°"""
    
    def __init__(self, tokenizer, use_ttsfrd=True):
        self.tokenizer = tokenizer
        self.use_ttsfrd = use_ttsfrd
        
        if self.use_ttsfrd:
            self.frd = ttsfrd.TtsFrontendEngine()
            # åˆå§‹åŒ– ttsfrd èµ„æºï¼ˆä½¿ç”¨é¡¹ç›®ä¸­çš„èµ„æºç›®å½•ï¼‰
            resource_dir = os.path.join(os.path.dirname(__file__), 'pretrained_models/CosyVoice-ttsfrd/resource')
            if os.path.exists(resource_dir):
                if not self.frd.initialize(resource_dir):
                    logging.warning(f"ttsfrd åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ wetext")
                    self.use_ttsfrd = False
                    self._init_wetext()
                else:
                    self.frd.set_lang_type('pinyinvg')
                    logging.info("âœ… ttsfrd åˆå§‹åŒ–æˆåŠŸ")
            else:
                logging.warning(f"ttsfrd èµ„æºç›®å½•ä¸å­˜åœ¨: {resource_dir}ï¼Œå°†ä½¿ç”¨ wetext")
                self.use_ttsfrd = False
                self._init_wetext()
        else:
            self._init_wetext()
    
    def _init_wetext(self):
        """åˆå§‹åŒ– wetext è§„èŒƒåŒ–å™¨ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        if 'ZhNormalizer' in globals():
            self.zh_tn_model = ZhNormalizer(remove_erhua=False)
            self.en_tn_model = EnNormalizer()
            self.inflect_parser = inflect.engine()
            logging.info("ä½¿ç”¨ wetext ä½œä¸ºæ–‡æœ¬è§„èŒƒåŒ–å·¥å…·")
        else:
            logging.warning("wetext ä¸å¯ç”¨ï¼Œå°†è·³è¿‡æ–‡æœ¬è§„èŒƒåŒ–")
    
    def normalize_and_split(self, text, token_max_n=80, token_min_n=60):
        """æ–‡æœ¬è§„èŒƒåŒ–ä¸æ™ºèƒ½åˆ†æ®µï¼ˆå¤ç”¨ frontend.text_normalize é€»è¾‘ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            token_max_n: æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ 80ï¼‰
            token_min_n: æœ€å° token æ•°ï¼ˆé»˜è®¤ 60ï¼‰
        
        Returns:
            List[str]: è§„èŒƒåŒ–å¹¶åˆ‡åˆ†åçš„æ–‡æœ¬æ®µè½åˆ—è¡¨
        """
        if not text or text.strip() == '':
            return []
        
        text = text.strip()
        
        # ä½¿ç”¨ ttsfrd è¿›è¡Œæ–‡æœ¬è§„èŒƒåŒ–
        if self.use_ttsfrd:
            try:
                result = self.frd.do_voicegen_frd(text)
                texts = [i["text"] for i in json.loads(result)["sentences"]]
                text = ''.join(texts)
            except Exception as e:
                logging.warning(f"ttsfrd å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
        else:
            # ä½¿ç”¨ wetext è¿›è¡Œè§„èŒƒåŒ–ï¼ˆä¸ frontend.py é€»è¾‘ä¸€è‡´ï¼‰
            if 'zh_tn_model' in dir(self):
                if contains_chinese(text):
                    text = self.zh_tn_model.normalize(text)
                    text = text.replace("\n", "")
                    text = replace_blank(text)
                    text = replace_corner_mark(text)
                    text = text.replace(".", "ã€‚")
                    text = text.replace(" - ", "ï¼Œ")
                    text = remove_bracket(text)
                    text = re.sub(r'[ï¼Œ,ã€]+$', 'ã€‚', text)
                else:
                    text = self.en_tn_model.normalize(text)
                    text = spell_out_number(text, self.inflect_parser)
        
        # ä½¿ç”¨ split_paragraph è¿›è¡Œæ™ºèƒ½åˆ†æ®µï¼ˆä¸ frontend.py é€»è¾‘ä¸€è‡´ï¼‰
        tokenize_fn = partial(self.tokenizer.encode, allowed_special='all')
        
        if contains_chinese(text):
            texts = list(split_paragraph(
                text, tokenize_fn, "zh", 
                token_max_n=token_max_n,
                token_min_n=token_min_n, 
                merge_len=20, 
                comma_split=False
            ))
        else:
            texts = list(split_paragraph(
                text, tokenize_fn, "en", 
                token_max_n=token_max_n,
                token_min_n=token_min_n, 
                merge_len=20, 
                comma_split=False
            ))
        
        # è¿‡æ»¤çº¯æ ‡ç‚¹æ®µè½
        texts = [i for i in texts if not is_only_punctuation(i)]
        
        if texts:
            logging.info(f"[æ–‡æœ¬è§„èŒƒåŒ–] åŸå§‹é•¿åº¦: {len(text)} å­—ç¬¦ â†’ åˆ†æˆ {len(texts)} æ®µ")
            for idx, seg in enumerate(texts):
                logging.info(f"  æ®µ{idx+1}: {len(seg)}å­—ç¬¦ - '{seg[:30]}{'...' if len(seg) > 30 else ''}'")
        
        return texts


class CosyVoiceServer:
    """CosyVoice2 æ¨ç†ç±»ï¼ˆçº¯ PyTorch å®ç°ï¼‰"""
    
    def __init__(self, cosyvoice_model, spk2info_path=None):
        self.cosyvoice: CosyVoice2 = cosyvoice_model
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.spk2info_path = spk2info_path
        
        # åˆå§‹åŒ–æ–‡æœ¬è§„èŒƒåŒ–å™¨
        self.text_normalizer = TextNormalizer(
            tokenizer=self.cosyvoice.frontend.tokenizer,
            use_ttsfrd=USE_TTSFRD
        )
    
    def inference_zero_shot(self, text, prompt_text, prompt_speech_16k, zero_shot_spk_id='', stream=True, request_start_time=None):
        """é›¶æ ·æœ¬æ¨ç†ï¼ˆPyTorch åŸç”Ÿå®ç° + é•¿æ–‡æœ¬åˆ†æ®µï¼‰"""
        # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´ï¼ˆå¦‚æœæœªä¼ å…¥ï¼‰
        if request_start_time is None:
            request_start_time = time.perf_counter()
        
        # ========== é•¿æ–‡æœ¬åˆ†æ®µé¢„å¤„ç† ==========
        text_segments = self.text_normalizer.normalize_and_split(
            text, 
            token_max_n=80,
            token_min_n=60
        )
        
        if len(text_segments) == 0:
            logging.warning("[æ–‡æœ¬åˆ†æ®µ] è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡æ¨ç†")
            return
        
        logging.info(f"[é•¿æ–‡æœ¬å¤„ç†] åŸå§‹æ–‡æœ¬ {len(text)} å­—ç¬¦ â†’ åˆ†æˆ {len(text_segments)} æ®µè¿›è¡Œæµå¼æ¨ç†")
        
        # ========== é€æ®µæ¨ç†å¹¶æµå¼è¿”å› ==========
        for segment_idx, text_segment in enumerate(text_segments):
            segment_start_time = time.perf_counter()
            logging.info(f"[æ®µè½æ¨ç† {segment_idx+1}/{len(text_segments)}] å¼€å§‹å¤„ç†: '{text_segment[:50]}{'...' if len(text_segment) > 50 else ''}'")
            
            # è°ƒç”¨å•æ®µæ¨ç†
            for output in self._inference_single_segment(
                text_segment, prompt_text, prompt_speech_16k,
                zero_shot_spk_id=zero_shot_spk_id,
                stream=stream,
                request_start_time=request_start_time if segment_idx == 0 else segment_start_time,
                is_first_segment=(segment_idx == 0)
            ):
                yield output
            
            segment_time = (time.perf_counter() - segment_start_time) * 1000
            logging.info(f"[æ®µè½æ¨ç† {segment_idx+1}/{len(text_segments)}] å®Œæˆï¼Œè€—æ—¶: {segment_time:.2f}ms")
    
    def _inference_single_segment(self, text, prompt_text, prompt_speech_16k, zero_shot_spk_id='', stream=True, request_start_time=None, is_first_segment=True):
        """å•æ®µæ–‡æœ¬æ¨ç†ï¼ˆPyTorch åŸç”Ÿå®ç°ï¼‰"""
        try:
            # ========== é˜¶æ®µ 1: ä¸Šä¸‹æ–‡åŠ è½½ ==========
            context_start = time.perf_counter()
            
            # ä½¿ç”¨ CosyVoice2 åŸç”Ÿçš„æµå¼æ¨ç†æ¥å£
            model_output = self.cosyvoice.inference_zero_shot(
                text, 
                prompt_text, 
                prompt_speech_16k, 
                zero_shot_spk_id=zero_shot_spk_id,
                stream=stream
            )
            
            context_load_time = (time.perf_counter() - context_start) * 1000
            if is_first_segment:
                logging.info(f"[å»¶è¿Ÿåˆ†æ-01] æ¨¡å‹æ¨ç†å¯åŠ¨: {context_load_time:.2f}ms")
            
            # ========== é˜¶æ®µ 2: æµå¼è¾“å‡º ==========
            is_first = True
            chunk_count = 0
            total_audio_duration = 0.0
            total_processing_time = 0.0
            sample_rate = 22050
            
            for output in model_output:
                chunk_start = time.perf_counter()
                
                if is_first and is_first_segment:
                    ttfb = (chunk_start - request_start_time) * 1000
                    logging.info(f"\n{'='*70}")
                    logging.info(f"[é¦–åŒ…å»¶è¿Ÿæ±‡æ€» TTFB] æ€»è€—æ—¶: {ttfb:.2f}ms")
                    logging.info(f"{'='*70}\n")
                    is_first = False
                
                tts_speech = output['tts_speech']
                
                # è®¡ç®—éŸ³é¢‘æ—¶é•¿å’Œ RTF
                chunk_audio_duration = tts_speech.shape[-1] / sample_rate
                chunk_processing_time = (time.perf_counter() - chunk_start)
                chunk_rtf = chunk_processing_time / chunk_audio_duration if chunk_audio_duration > 0 else 0
                
                total_audio_duration += chunk_audio_duration
                total_processing_time += chunk_processing_time
                cumulative_rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0
                
                chunk_count += 1
                logging.info(f"[æµå¼è¾“å‡º] å—{chunk_count}: éŸ³é¢‘={chunk_audio_duration*1000:.0f}ms, RTF={chunk_rtf:.3f}, ç´¯ç§¯RTF={cumulative_rtf:.3f}")
                
                yield {'tts_speech': tts_speech.cpu()}
            
            # è¾“å‡ºæ€»ä½“ RTF ç»Ÿè®¡
            overall_rtf = total_processing_time / total_audio_duration if total_audio_duration > 0 else 0
            if is_first_segment:
                logging.info(f"[æ•´ä½“RTFç»Ÿè®¡] æ€»éŸ³é¢‘æ—¶é•¿: {total_audio_duration:.2f}s, æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f}s, æ•´ä½“RTF: {overall_rtf:.3f}")
            
        except Exception as e:
            logging.error(f"PyTorch æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def list_available_spks(self):
        """è·å–å¯ç”¨è¯´è¯äººåˆ—è¡¨"""
        return self.cosyvoice.list_available_spks()
    
    def add_zero_shot_spk(self, prompt_text, prompt_speech_16k, zero_shot_spk_id):
        """æ·»åŠ é›¶æ ·æœ¬è¯´è¯äºº"""
        return self.cosyvoice.add_zero_shot_spk(prompt_text, prompt_speech_16k, zero_shot_spk_id)
    
    def save_spkinfo(self):
        """ä¿å­˜è¯´è¯äººä¿¡æ¯åˆ°æŒ‡å®šè·¯å¾„"""
        if self.spk2info_path:
            torch.save(self.cosyvoice.frontend.spk2info, self.spk2info_path)
            logging.info(f"è¯´è¯äººä¿¡æ¯å·²ä¿å­˜åˆ°: {self.spk2info_path}")
        else:
            return self.cosyvoice.save_spkinfo()


def generate_data(model_output, request_start_time):
    """ç”ŸæˆéŸ³é¢‘æ•°æ®æµï¼Œå¯¹è¾“å‡ºè¿›è¡Œå‰Šæ³¢å¤„ç†é˜²æ­¢çˆ†éŸ³"""
    is_first = True
    chunk_count = 0
    
    for i in model_output:
        if is_first:
            first_chunk_time = time.perf_counter()
            ttfb = (first_chunk_time - request_start_time) * 1000
            logging.info(f"[TTSç»Ÿè®¡] HTTPå“åº”é¦–åŒ…ç”Ÿæˆå®Œæ¯•! HTTP TTFB: {ttfb:.2f}ms")
            is_first = False

        tts_speech = i["tts_speech"].numpy()
        
        # è¾“å‡ºç«¯å‰Šæ³¢ï¼šé˜²æ­¢ float -> int16 è½¬æ¢æ—¶çš„æ•´æ•°æº¢å‡º
        tts_speech = np.clip(tts_speech, -1.0, 1.0)
        
        # è½¬æ¢ä¸º int16 æ ¼å¼
        tts_audio = (tts_speech * 32767.0).astype(np.int16).tobytes()
        chunk_count += 1
        yield tts_audio
    
    total_time = (time.perf_counter() - request_start_time) * 1000
    logging.info(f"[TTSç»Ÿè®¡] æµå¼ä¼ è¾“ç»“æŸ. æ€»è€—æ—¶: {total_time:.2f}ms, å…±å‘é€ {chunk_count} ä¸ªæ•°æ®å—")


def load_speakers_from_directory(speaker_dir="asset/speakers"):
    """ä»ç›®å½•åŠ è½½æ‰€æœ‰è¯´è¯äºº"""
    speakers = {}
    
    if not os.path.exists(speaker_dir):
        logging.warning(f"è¯´è¯äººç›®å½• {speaker_dir} ä¸å­˜åœ¨")
        return speakers
    
    wav_files = glob.glob(os.path.join(speaker_dir, "*.wav"))
    
    for wav_path in wav_files:
        filename = os.path.basename(wav_path)
        # è§£ææ–‡ä»¶åæ ¼å¼ï¼š[è¯´è¯äººåç§°]æ–‡æœ¬å†…å®¹.wav
        match = re.match(r'\[(.+?)\](.+)\.wav$', filename)
        
        if match:
            speaker_name = match.group(1)
            prompt_text = match.group(2)
            
            try:
                prompt_speech_16k = load_wav(wav_path, 16000)
                
                # è¾“å…¥ç«¯å½’ä¸€åŒ–
                max_val = torch.abs(prompt_speech_16k).max().item()
                target_peak = 0.95
                
                if max_val > target_peak:
                    logging.warning(f"è¯´è¯äºº {speaker_name} éŸ³é¢‘å³°å€¼ {max_val:.4f} è¶…å‡ºå®‰å…¨èŒƒå›´ï¼Œå½’ä¸€åŒ–åˆ° {target_peak}")
                    prompt_speech_16k = (prompt_speech_16k / max_val) * target_peak
                
                speakers[speaker_name] = {
                    'prompt_text': prompt_text,
                    'prompt_speech_16k': prompt_speech_16k,
                    'wav_path': wav_path
                }
                logging.info(f"åŠ è½½è¯´è¯äºº: {speaker_name}")
            except Exception as e:
                logging.error(f"åŠ è½½è¯´è¯äºº {speaker_name} å¤±è´¥: {e}")
        else:
            logging.warning(f"æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡: {filename}")
    
    return speakers


@app.get("/")
async def index():
    """ä¸»é¡µè·¯ç”±ï¼Œè¿”å›å‰ç«¯é¡µé¢"""
    index_path = os.path.join(os.path.dirname(__file__), "static", "index.html")
    if os.path.exists(index_path):
        return FileResponse(index_path)
    return {"message": "CosyVoice TTS Server is running. Visit /static/index.html for the web interface."}


@app.get("/api/speakers")
async def get_speakers():
    """è·å–æ‰€æœ‰å¯ç”¨çš„è¯´è¯äººåˆ—è¡¨"""
    try:
        speakers = cosyvoice_server.list_available_spks()
        return JSONResponse(content={"speakers": speakers})
    except Exception as e:
        logging.error(f"è·å–è¯´è¯äººåˆ—è¡¨å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/tts")
async def inference_zero_shot(request: Request, text: str = Form(), speaker: str = Form(default="")):
    """æ–‡æœ¬è½¬è¯­éŸ³æ¥å£ï¼ˆPyTorch åŸç”Ÿå®ç°ï¼‰"""
    # ä½¿ç”¨ä¸­é—´ä»¶è®°å½•çš„å¼€å§‹æ—¶é—´ï¼Œç¡®ä¿ä¸å‰ç«¯å¯¹é½
    request_start_time = request.state.start_time
    logging.info(f"[TTSè¯·æ±‚] æ”¶åˆ°è¯·æ±‚: text='{text[:50] if len(text) > 50 else text}', speaker='{speaker}'")

    try:
        # è·å–å¯ç”¨è¯´è¯äººåˆ—è¡¨
        available_spks = cosyvoice_server.list_available_spks()
        # é»˜è®¤ä½¿ç”¨jokè€å¸ˆï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªè¯´è¯äºº
        default_speaker = "jokè€å¸ˆ" if "jokè€å¸ˆ" in available_spks else (available_spks[0] if available_spks else "")
        selected_speaker = speaker if speaker else default_speaker
        
        if not selected_speaker:
            return JSONResponse(content={"error": "æ²¡æœ‰å¯ç”¨çš„è¯´è¯äºº"}, status_code=400)
        
        logging.info(f"[TTSæ¨ç†] å¼€å§‹æ¨ç†, è¯´è¯äºº: {selected_speaker}")

        # ä½¿ç”¨ CosyVoiceServer è¿›è¡Œæ¨ç†
        model_output = cosyvoice_server.inference_zero_shot(
            text, "", None, 
            zero_shot_spk_id=selected_speaker,
            stream=True,
            request_start_time=request_start_time
        )
        return StreamingResponse(generate_data(model_output, request_start_time))
    except Exception as e:
        logging.error(f"TTSæ¨ç†å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=50000, help="æœåŠ¡ç«¯å£")
    parser.add_argument(
        "--model_dir",
        type=str,
        default="pretrained_models/CosyVoice2-0.5B",
        help="æ¨¡å‹æœ¬åœ°è·¯å¾„æˆ– modelscope ä»“åº“ id",
    )
    parser.add_argument(
        "--speaker_dir",
        type=str,
        default="asset/speakers",
        help="è¯´è¯äººéŸ³é¢‘æ–‡ä»¶ç›®å½•",
    )
    args = parser.parse_args()

    try:
        # åˆå§‹åŒ– CosyVoice2 æ¨¡å‹
        logging.info("=" * 60)
        logging.info("å¯åŠ¨ CosyVoice TTS Server")
        logging.info(f"æ¨¡å‹ç›®å½•: {args.model_dir}")
        logging.info("=" * 60)
        
        # åŠ è½½ CosyVoice2 æ¨¡å‹ï¼ˆå¯ç”¨æ‰€æœ‰åŠ é€Ÿé€‰é¡¹ï¼‰
        cosyvoice = CosyVoice2(
            args.model_dir, 
            load_jit=True,   # âœ… JITç¼–è¯‘åŠ é€Ÿ flow.encoder
            load_trt=True,   # âœ… TensorRTä¼˜åŒ–
            fp16=True        # âœ… FP16æ··åˆç²¾åº¦
        )
        logging.info("âœ… æ¨¡å‹åŠ è½½é…ç½®: JIT=True, TRT=True, FP16=True")
        
        # åˆ›å»º CosyVoiceServer å®ä¾‹
        spk2info_path = os.path.join(args.speaker_dir, 'spk2info.pt')
        cosyvoice_server = CosyVoiceServer(
            cosyvoice_model=cosyvoice,
            spk2info_path=spk2info_path,
        )
        logging.info("âœ… CosyVoiceServer åˆå§‹åŒ–æˆåŠŸ")

    except Exception as e:
        raise TypeError(f"å¯¼å…¥{args.model_dir}å¤±è´¥ï¼Œæ¨¡å‹ç±»å‹æœ‰è¯¯ï¼é”™è¯¯: {e}")

    # åŠ è½½è¯´è¯äººä¿¡æ¯
    spk2info_path = os.path.join(args.speaker_dir, 'spk2info.pt')
    
    # æ£€æŸ¥ speakers ç›®å½•ä¸‹çš„ spk2info.pt æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ‰€æœ‰è¯´è¯äºº
    need_regenerate = False
    
    # å¦‚æœ speakers ç›®å½•ä¸‹æœ‰ spk2info.ptï¼ŒåŠ è½½å®ƒ
    if os.path.exists(spk2info_path):
        spk2info_data = torch.load(spk2info_path, map_location=cosyvoice_server.device)
        cosyvoice_server.cosyvoice.frontend.spk2info.update(spk2info_data)
        logging.info(f"å·²åŠ è½½ spk2info.pt: {spk2info_path}")
        print(f"spk2info.pt å·²å­˜åœ¨ï¼ŒåŒ…å« {len(spk2info_data)} ä¸ªè¯´è¯äºº")
    else:
        print(f"æœªæ‰¾åˆ° spk2info.ptï¼Œå°†ç”Ÿæˆæ–°æ–‡ä»¶")
        need_regenerate = True
    
    if need_regenerate:
        print("æ­£åœ¨æå–è¯´è¯äººéŸ³é¢‘ç‰¹å¾...")
        speakers_data = load_speakers_from_directory(args.speaker_dir)
        
        if not speakers_data:
            print(f"è­¦å‘Šï¼šæœªåœ¨ {args.speaker_dir} ç›®å½•æ‰¾åˆ°ä»»ä½•è¯´è¯äººæ–‡ä»¶")
        else:
            print(f"æˆåŠŸåŠ è½½ {len(speakers_data)} ä¸ªè¯´è¯äºº")
            
            # å°†æ‰€æœ‰è¯´è¯äººæ·»åŠ åˆ°æ¨¡å‹
            for speaker_name, speaker_info in speakers_data.items():
                try:
                    cosyvoice_server.add_zero_shot_spk(
                        speaker_info['prompt_text'],
                        speaker_info['prompt_speech_16k'],
                        speaker_name
                    )
                    print(f"  âœ“ {speaker_name}")
                except Exception as e:
                    print(f"  âœ— {speaker_name}: {e}")
            
            # ä¿å­˜è¯´è¯äººä¿¡æ¯
            try:
                # ç¡®ä¿ speaker_dir å­˜åœ¨
                os.makedirs(args.speaker_dir, exist_ok=True)
                cosyvoice_server.save_spkinfo()
                print(f"è¯´è¯äººä¿¡æ¯å·²ä¿å­˜åˆ° {spk2info_path}")
            except Exception as e:
                print(f"ä¿å­˜è¯´è¯äººä¿¡æ¯å¤±è´¥: {e}")
    
    # æ¨¡å‹é¢„çƒ­
    print("\næ­£åœ¨é¢„çƒ­æ¨¡å‹...")
    available_spks = cosyvoice_server.list_available_spks()
    if available_spks:
        warmup_speaker = "jokè€å¸ˆ" if "jokè€å¸ˆ" in available_spks else available_spks[0]
        print(f"ä½¿ç”¨ '{warmup_speaker}' è¿›è¡Œé¢„çƒ­")
        warmup_texts = [
            "ä½ å¥½ã€‚",  # çŸ­å¥
            "è¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„çƒ­æ¨¡å‹çš„æµ‹è¯•å¥å­ï¼Œç¡®ä¿æœåŠ¡å“åº”é€Ÿåº¦ã€‚", # ä¸­å¥
            "è¯­éŸ³åˆæˆæœåŠ¡æ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨å€™ï¼Œç³»ç»Ÿæ­£åœ¨è¿›è¡Œåˆå§‹åŒ–æ“ä½œã€‚", # é•¿å¥
            "å¥½çš„ï¼Œæ²¡é—®é¢˜ã€‚" # çŸ­å¥
        ]
        
        for t in warmup_texts:
            try:
                for _ in cosyvoice_server.inference_zero_shot(
                        t, "", None, zero_shot_spk_id=warmup_speaker, stream=True):
                    pass
            except Exception as e:
                print(f"é¢„çƒ­å¤±è´¥: {e}")
                break
    else:
        print("æœªæ‰¾åˆ°å¯ç”¨è¯´è¯äººï¼Œè·³è¿‡é¢„çƒ­")
    
    print("é¢„çƒ­å®Œæ¯•\n")
    print("=" * 60)
    print(f"ğŸš€ CosyVoice TTS Server å¯åŠ¨åœ¨ç«¯å£ {args.port}")

    # é…ç½® uvicorn ä»¥æ”¯æŒ HTTP keep-alive
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=args.port,
        timeout_keep_alive=60,  # keep-alive è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        limit_concurrency=100,  # æœ€å¤§å¹¶å‘è¿æ¥æ•°
        backlog=2048,  # TCP backlog é˜Ÿåˆ—å¤§å°
    )
