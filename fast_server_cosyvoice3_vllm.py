import os
import time
import sys
import argparse
import logging
import re
import glob
import json
from functools import partial
import inflect

# é…ç½®æ—¥å¿—æ ¼å¼ï¼Œç¡®ä¿æ˜¾ç¤ºæ¯«ç§’çº§æ—¶é—´æˆ³
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s.%(msecs)03d %(levelname)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logging.getLogger("matplotlib").setLevel(logging.WARNING)

from fastapi import FastAPI, Form, Request
from fastapi.responses import StreamingResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
import uvicorn
import numpy as np
import torch
import torchaudio

# æ·»åŠ  Matcha-TTS è·¯å¾„
sys.path.append("third_party/Matcha-TTS")

# Monkey patch load_wav å‡½æ•°ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ CosyVoice ä¹‹å‰ï¼‰
import cosyvoice.utils.file_utils

def patched_load_wav(wav, target_sr, min_sr=16000):
    """ä½¿ç”¨ soundfile æ›¿ä»£ torchaudio.load ä»¥å…¼å®¹ PyTorch 2.9.x"""
    import soundfile as sf
    speech, sample_rate = sf.read(wav, dtype='float32')
    # soundfileè¿”å› (samples,) æˆ– (samples, channels)ï¼Œè½¬ä¸º (channels, samples)
    if len(speech.shape) == 1:
        speech = torch.from_numpy(speech).unsqueeze(0)  # (samples,) -> (1, samples)
    else:
        speech = torch.from_numpy(speech).T  # (samples, channels) -> (channels, samples)
    speech = speech.mean(dim=0, keepdim=True)
    if sample_rate != target_sr:
        assert sample_rate >= min_sr, f'wav sample rate {sample_rate} must be greater than {target_sr}'
        speech = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)(speech)
    return speech

# åº”ç”¨ patchï¼ˆåœ¨å¯¼å…¥å…¶ä»–æ¨¡å—å‰ï¼‰
cosyvoice.utils.file_utils.load_wav = patched_load_wav

# å¯¼å…¥ vLLM å’Œ CosyVoice
from vllm import ModelRegistry
from cosyvoice.vllm.cosyvoice3 import CosyVoice3ForCausalLM
from cosyvoice.utils.frontend_utils import (
    contains_chinese,
    replace_blank,
    replace_corner_mark,
    remove_bracket,
    spell_out_number,
    split_paragraph,
    is_only_punctuation,
)
import uuid as uuid_module

# æ³¨å†Œ CosyVoice3 æ¨¡å‹åˆ° vLLM
ModelRegistry.register_model("CosyVoice3ForCausalLM", CosyVoice3ForCausalLM)

# å¯¼å…¥ ttsfrd æ¨¡å—ï¼ˆç”¨äºæ–‡æœ¬è§„èŒƒåŒ–ï¼‰
try:
    import ttsfrd
    USE_TTSFRD = True
    logging.info("å·²å¯¼å…¥ ttsfrd æ¨¡å—ç”¨äºæ–‡æœ¬è§„èŒƒåŒ–")
except ImportError:
    USE_TTSFRD = False
    logging.warning("ttsfrd ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ wetext è¿›è¡Œæ–‡æœ¬è§„èŒƒåŒ–")
    try:
        from wetext import Normalizer as ZhNormalizer
        from wetext import Normalizer as EnNormalizer
    except ImportError:
        logging.warning("wetext ä¹Ÿä¸å¯ç”¨ï¼Œæ–‡æœ¬è§„èŒƒåŒ–åŠŸèƒ½å°†å—é™")

app = FastAPI()


class TimingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        request.state.start_time = time.perf_counter()
        response = await call_next(request)
        return response


app.add_middleware(TimingMiddleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
static_dir = os.path.join(os.path.dirname(__file__), "static")
if os.path.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")


class TextNormalizer:
    """æ–‡æœ¬è§„èŒƒåŒ–å·¥å…·ç±»ï¼Œå¤ç”¨ CosyVoice frontend çš„æˆç†Ÿå®ç°"""

    def __init__(self, tokenizer, use_ttsfrd=True):
        self.tokenizer = tokenizer
        self.use_ttsfrd = use_ttsfrd
        if self.use_ttsfrd:
            self.frd = ttsfrd.TtsFrontendEngine()
            resource_dir = os.path.join(
                os.path.dirname(__file__), "pretrained_models/CosyVoice-ttsfrd/resource"
            )
            if os.path.exists(resource_dir):
                if not self.frd.initialize(resource_dir):
                    logging.warning("ttsfrd åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ wetext")
                    self.use_ttsfrd = False
                    self._init_wetext()
                else:
                    self.frd.set_lang_type("pinyinvg")
                    logging.info("âœ… ttsfrd åˆå§‹åŒ–æˆåŠŸ")
            else:
                logging.warning(
                    f"ttsfrd èµ„æºç›®å½•ä¸å­˜åœ¨: {resource_dir}ï¼Œå°†ä½¿ç”¨ wetext"
                )
                self.use_ttsfrd = False
                self._init_wetext()
        else:
            self._init_wetext()

    def _init_wetext(self):
        if "ZhNormalizer" in globals():
            self.zh_tn_model = ZhNormalizer(remove_erhua=False)
            self.en_tn_model = EnNormalizer()
            self.inflect_parser = inflect.engine()
            logging.info("ä½¿ç”¨ wetext ä½œä¸ºæ–‡æœ¬è§„èŒƒåŒ–å·¥å…·")
        else:
            logging.warning("wetext ä¸å¯ç”¨ï¼Œå°†è·³è¿‡æ–‡æœ¬è§„èŒƒåŒ–")

    def normalize_and_split(self, text, token_max_n=80, token_min_n=60):
        if not text or text.strip() == "":
            return []
        text = text.strip()
        if self.use_ttsfrd:
            try:
                result = self.frd.do_voicegen_frd(text)
                texts = [i["text"] for i in json.loads(result)["sentences"]]
                text = "".join(texts)
            except Exception as e:
                logging.warning(f"ttsfrd å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
        else:
            if "zh_tn_model" in dir(self):
                if contains_chinese(text):
                    text = self.zh_tn_model.normalize(text)
                    text = text.replace("\n", "")
                    text = replace_blank(text)
                    text = replace_corner_mark(text)
                    text = text.replace(".", "ã€‚")
                    text = text.replace(" - ", "ï¼Œ")
                    text = remove_bracket(text)
                    text = re.sub(r"[ï¼Œ,ã€]+$", "ã€‚", text)
                else:
                    text = self.en_tn_model.normalize(text)
                    text = spell_out_number(text, self.inflect_parser)

        tokenize_fn = partial(self.tokenizer.encode, allowed_special="all")
        if contains_chinese(text):
            texts = list(
                split_paragraph(
                    text,
                    tokenize_fn,
                    "zh",
                    token_max_n=token_max_n,
                    token_min_n=token_min_n,
                    merge_len=20,
                    comma_split=False,
                )
            )
        else:
            texts = list(
                split_paragraph(
                    text,
                    tokenize_fn,
                    "en",
                    token_max_n=token_max_n,
                    token_min_n=token_min_n,
                    merge_len=20,
                    comma_split=False,
                )
            )
        texts = [i for i in texts if not is_only_punctuation(i)]
        if texts:
            logging.info(
                f"[æ–‡æœ¬è§„èŒƒåŒ–] åŸå§‹é•¿åº¦: {len(text)} å­—ç¬¦ â†’ åˆ†æˆ {len(texts)} æ®µ"
            )
            for idx, seg in enumerate(texts):
                logging.info(
                    f"  æ®µ{idx+1}: {len(seg)}å­—ç¬¦ - '"
                    f"{seg[:30]}{'...' if len(seg) > 30 else ''}'"
                )
        return texts


def convert_speech_tokens_to_str(speech_tokens):
    if isinstance(speech_tokens, torch.Tensor):
        speech_tokens = speech_tokens.flatten().tolist()
    return "".join([f"<|s_{token}|>" for token in speech_tokens])


def extract_speech_ids_from_str(speech_tokens_str_list):
    speech_ids = []
    for token_str in speech_tokens_str_list:
        if token_str.startswith("<|s_") and token_str.endswith("|>"):
            try:
                num_str = token_str[4:-2]
                num = int(num_str)
                speech_ids.append(num)
            except ValueError:
                logging.warning(f"æ— æ³•è§£æ speech token: {token_str}")
    return speech_ids


class FastCosyVoice3VLLM:
    """é›†æˆ vLLM çš„ CosyVoice3 æ¨ç†ç±»"""

    def __init__(self, cosyvoice_model, vllm_tokenizer):
        """
        Args:
            cosyvoice_model: ä½¿ç”¨ AutoModel(load_vllm=True) åŠ è½½çš„æ¨¡å‹
            vllm_tokenizer: vLLM ä½¿ç”¨çš„ tokenizerï¼ˆç”¨äºæ–‡æœ¬è§„èŒƒåŒ–ï¼‰
        """
        self.cosyvoice = cosyvoice_model
        self.vllm_tokenizer = vllm_tokenizer
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.text_normalizer = TextNormalizer(
            tokenizer=self.vllm_tokenizer, use_ttsfrd=USE_TTSFRD
        )
        logging.info("âœ… FastCosyVoice3VLLM åˆå§‹åŒ–æˆåŠŸ")

    def inference_zero_shot(self, text, prompt_text, prompt_speech_16k, zero_shot_spk_id="", stream=True, request_start_time=None):
        """
        ä½¿ç”¨ vLLM åŠ é€Ÿçš„ zero-shot æ¨ç†
            
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç›´æ¥ä½¿ç”¨ cosyvoice.inference_zero_shotï¼Œå…¶å†…éƒ¨å·²ç»é›†æˆäº† vLLM
        å…³é”®ä¿®å¤ï¼šå½“ä½¿ç”¨ zero_shot_spk_id æ—¶ï¼Œå¿…é¡»å°† prompt_text å’Œ prompt_speech_16k è®¾ç½®ä¸ºç©º/None
        """
        if request_start_time is None:
            request_start_time = time.perf_counter()
            
        text_segments = self.text_normalizer.normalize_and_split(
            text, token_max_n=80, token_min_n=60
        )
        if len(text_segments) == 0:
            logging.warning("[æ–‡æœ¬åˆ†æ®µ] è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡æ¨ç†")
            return
            
        logging.info(
            f"[é•¿æ–‡æœ¬å¤„ç†] åŸå§‹æ–‡æœ¬ {len(text)} å­—ç¬¦ â†’ åˆ†æˆ {len(text_segments)} æ®µè¿›è¡Œæµå¼æ¨ç†"
        )
            
        for segment_idx, text_segment in enumerate(text_segments):
            segment_start_time = time.perf_counter()
            logging.info(
                f"[æ®µè½æ¨ç† {segment_idx+1}/{len(text_segments)}] å¼€å§‹å¤„ç†: '"
                f"{text_segment[:50]}{'...' if len(text_segment) > 50 else ''}"
            )
                
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæŒ‰ç…§ test_streaming_mvp.py çš„é€»è¾‘ï¼Œä½¿ç”¨ zero_shot_spk_id æ—¶å¿…é¡»æ¸…ç©º prompt å‚æ•°
            # å› ä¸ºè¯´è¯äººç‰¹å¾å·²ç»é€šè¿‡ add_zero_shot_spk æ³¨å†Œï¼Œå†ä¼ å…¥ prompt ä¼šå¯¼è‡´éŸ³é¢‘å¼‚å¸¸
            for output in self.cosyvoice.inference_zero_shot(
                text_segment,
                '',  # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯åŸå§‹ prompt_text
                None,  # ä½¿ç”¨ None è€Œä¸æ˜¯åŸå§‹ prompt_speech_16k
                zero_shot_spk_id=zero_shot_spk_id,
                stream=stream
            ):
                yield output
                
            segment_time = (time.perf_counter() - segment_start_time) * 1000
            logging.info(
                f"[æ®µè½æ¨ç† {segment_idx+1}/{len(text_segments)}] å®Œæˆï¼Œè€—æ—¶: {segment_time:.2f}ms"
            )

    def list_available_spks(self):
        return self.cosyvoice.list_available_spks()

    def add_zero_shot_spk(self, prompt_text, prompt_speech_16k, zero_shot_spk_id):
        return self.cosyvoice.add_zero_shot_spk(
            prompt_text, prompt_speech_16k, zero_shot_spk_id
        )


def generate_data(model_output, request_start_time):
    is_first = True
    chunk_count = 0
    for i in model_output:
        if is_first:
            first_chunk_time = time.perf_counter()
            ttfb = (first_chunk_time - request_start_time) * 1000
            logging.info(
                f"[TTSç»Ÿè®¡] HTTPå“åº”é¦–åŒ…ç”Ÿæˆå®Œæ¯•! HTTP TTFB: {ttfb:.2f}ms"
            )
            is_first = False
        tts_speech = i["tts_speech"].numpy()
        tts_speech = np.clip(tts_speech, -1.0, 1.0)
        tts_audio = (tts_speech * 32767.0).astype(np.int16).tobytes()
        chunk_count += 1
        yield tts_audio
    total_time = (time.perf_counter() - request_start_time) * 1000
    logging.info(
        f"[TTSç»Ÿè®¡] æµå¼ä¼ è¾“ç»“æŸ. æ€»è€—æ—¶: {total_time:.2f}ms, å…±å‘é€ {chunk_count} ä¸ªæ•°æ®å—"
    )


def load_speakers_from_directory(speaker_dir="asset/speakers"):
    speakers = {}
    if not os.path.exists(speaker_dir):
        logging.warning(f"è¯´è¯äººç›®å½• {speaker_dir} ä¸å­˜åœ¨")
        return speakers
    wav_files = glob.glob(os.path.join(speaker_dir, "*.wav"))
    for wav_path in wav_files:
        filename = os.path.basename(wav_path)
        match = re.match(r"\[(.+?)\](.+)\.wav$", filename)
        if match:
            speaker_name = match.group(1)
            prompt_text_raw = match.group(2)
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸º CosyVoice3 æ·»åŠ ç³»ç»Ÿæç¤ºè¯å‰ç¼€
            prompt_text = f"You are a helpful assistant.<|endofprompt|>{prompt_text_raw}"
            try:
                prompt_speech_16k = patched_load_wav(wav_path, 16000)
                max_val = torch.abs(prompt_speech_16k).max().item()
                target_peak = 0.95
                if max_val > target_peak:
                    logging.warning(
                        f"è¯´è¯äºº {speaker_name} éŸ³é¢‘å³°å€¼ {max_val:.4f} è¶…å‡ºå®‰å…¨èŒƒå›´ï¼Œå½’ä¸€åŒ–åˆ° {target_peak}"
                    )
                    prompt_speech_16k = (prompt_speech_16k / max_val) * target_peak
                speakers[speaker_name] = {
                    "prompt_text": prompt_text,
                    "prompt_speech_16k": prompt_speech_16k,
                    "wav_path": wav_path,
                }
                logging.info(f"åŠ è½½è¯´è¯äºº: {speaker_name} (prompt_text='{prompt_text}')")
            except Exception as e:
                logging.error(f"åŠ è½½è¯´è¯äºº {speaker_name} å¤±è´¥: {e}")
        else:
            logging.warning(f"æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡: {filename}")
    return speakers



@app.get("/")
async def index():
    index_path = os.path.join(os.path.dirname(__file__), "static", "index.html")
    if os.path.exists(index_path):
        return FileResponse(index_path)
    return {
        "message": "FastCosyVoice3 vLLM TTS Server is running. Visit /static/index.html for the web interface.",
    }


@app.get("/api/speakers")
async def get_speakers():
    try:
        speakers = fast_cosyvoice.list_available_spks()
        return JSONResponse(content={"speakers": speakers})
    except Exception as e:
        logging.error(f"è·å–è¯´è¯äººåˆ—è¡¨å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/tts")
async def inference_zero_shot(request: Request, text: str = Form(), speaker: str = Form(default="")):
    request_start_time = request.state.start_time
    logging.info(
        f"[FastTTSè¯·æ±‚] æ”¶åˆ°è¯·æ±‚: text='{text[:50] if len(text) > 50 else text}', speaker='{speaker}'"
    )
    try:
        available_spks = fast_cosyvoice.list_available_spks()
        default_speaker = (
            "jokè€å¸ˆ" if "jokè€å¸ˆ" in available_spks else (available_spks[0] if available_spks else "")
        )
        selected_speaker = speaker if speaker else default_speaker
        if not selected_speaker:
            return JSONResponse(content={"error": "æ²¡æœ‰å¯ç”¨çš„è¯´è¯äºº"}, status_code=400)
        logging.info(f"[FastTTSæ¨ç†] å¼€å§‹æ¨ç†, è¯´è¯äºº: {selected_speaker}")
        model_output = fast_cosyvoice.inference_zero_shot(
            text,
            "",
            None,
            zero_shot_spk_id=selected_speaker,
            stream=True,
            request_start_time=request_start_time,
        )
        return StreamingResponse(generate_data(model_output, request_start_time))
    except Exception as e:
        logging.error(f"FastTTSæ¨ç†å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=50002, help="æœåŠ¡ç«¯å£")
    parser.add_argument(
        "--model_dir",
        type=str,
        default="pretrained_models/Fun-CosyVoice3-0.5B",
        help="æ¨¡å‹æœ¬åœ°è·¯å¾„æˆ– modelscope ä»“åº“ id",
    )
    parser.add_argument(
        "--speaker_dir",
        type=str,
        default="asset/speakers",
        help="è¯´è¯äººéŸ³é¢‘æ–‡ä»¶ç›®å½•",
    )
    args = parser.parse_args()

    try:
        logging.info("=" * 60)
        logging.info("å¯åŠ¨ FastCosyVoice3 vLLM TTS Server")
        logging.info(f"æ¨¡å‹ç›®å½•: {args.model_dir}")
        logging.info(f"vLLM ç‰ˆæœ¬: 0.12.0 (é€‚é… PyTorch 2.9.x + RTX 50 ç³»åˆ—)")
        logging.info("=" * 60)

        # å¯¼å…¥ AutoModel
        from cosyvoice.cli.cosyvoice import AutoModel
        
        # åŠ è½½ CosyVoice3 æ¨¡å‹ï¼ˆå¯ç”¨ vLLMï¼‰
        logging.info("å¼€å§‹åŠ è½½ CosyVoice3 æ¨¡å‹ï¼ˆvLLM 0.12.0 + PyTorch 2.9.1ï¼‰...")
        cosyvoice = AutoModel(
            model_dir=args.model_dir,
            load_trt=True,
            load_vllm=True,
            fp16=False
        )
        logging.info("âœ… æ¨¡å‹åŠ è½½é…ç½®: vLLM=True, TRT=True, FP16=False")

        # åˆ›å»º FastCosyVoice3VLLM å®ä¾‹
        # è·å– vLLM tokenizerï¼ˆç”¨äºæ–‡æœ¬è§„èŒƒåŒ–ï¼‰
        if hasattr(cosyvoice, 'frontend') and hasattr(cosyvoice.frontend, 'tokenizer'):
            vllm_tokenizer = cosyvoice.frontend.tokenizer
        else:
            # å¦‚æœè·å–ä¸åˆ°ï¼Œä½¿ç”¨ transformers AutoTokenizer
            from transformers import AutoTokenizer
            tokenizer_path = os.path.join(args.model_dir, "llm")
            vllm_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
            logging.info(f"ä» {tokenizer_path} åŠ è½½ tokenizer")
        
        fast_cosyvoice = FastCosyVoice3VLLM(
            cosyvoice_model=cosyvoice,
            vllm_tokenizer=vllm_tokenizer,
        )
        logging.info("âœ… FastCosyVoice3VLLM åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ vLLM åŠ é€Ÿ")

    except Exception as e:
        raise TypeError(f"å¯¼å…¥{args.model_dir}å¤±è´¥ï¼Œæ¨¡å‹ç±»å‹æœ‰è¯¯ï¼é”™è¯¯: {e}")

    # ğŸ”¥ å¼ºåˆ¶æ¯æ¬¡å¯åŠ¨éƒ½é‡æ–°æå–ç‰¹å¾ï¼ˆä¸ä½¿ç”¨ spk2info.pt ç¼“å­˜ï¼‰
    print("=" * 60)
    print("âš ï¸  ç¦ç”¨ spk2info.pt ç¼“å­˜ï¼Œæ¯æ¬¡å¯åŠ¨éƒ½é‡æ–°æå–è¯´è¯äººç‰¹å¾")
    print("=" * 60)
    
    print("æ­£åœ¨ä» wav æ–‡ä»¶æå–è¯´è¯äººéŸ³é¢‘ç‰¹å¾...")
    speakers_data = load_speakers_from_directory(args.speaker_dir)
    if not speakers_data:
        print(f"è­¦å‘Šï¼šæœªåœ¨ {args.speaker_dir} ç›®å½•æ‰¾åˆ°ä»»ä½•è¯´è¯äººæ–‡ä»¶")
    else:
        print(f"æˆåŠŸåŠ è½½ {len(speakers_data)} ä¸ªè¯´è¯äºº")
        for speaker_name, speaker_info in speakers_data.items():
            try:
                # ä½¿ç”¨ wav_path è€Œä¸æ˜¯ tensor
                fast_cosyvoice.add_zero_shot_spk(
                    speaker_info["prompt_text"],
                    speaker_info["wav_path"],  # ä¼ å…¥æ–‡ä»¶è·¯å¾„
                    speaker_name,
                )
                print(f"  âœ“ {speaker_name}")
            except Exception as e:
                print(f"  âœ— {speaker_name}: {e}")

    print("\næ­£åœ¨é¢„çƒ­æ¨¡å‹...")
    available_spks = fast_cosyvoice.list_available_spks()
    if available_spks:
        warmup_speaker = "jokè€å¸ˆ" if "jokè€å¸ˆ" in available_spks else available_spks[0]
        print(f"ä½¿ç”¨ '{warmup_speaker}' è¿›è¡Œé¢„çƒ­")
        warmup_texts = [
            "ä½ å¥½ã€‚",
            "è¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„çƒ­æ¨¡å‹çš„æµ‹è¯•å¥å­ï¼Œç¡®ä¿æœåŠ¡å“åº”é€Ÿåº¦ã€‚",
        ]
        for t in warmup_texts:
            try:
                for _ in fast_cosyvoice.inference_zero_shot(
                    t, "", None, zero_shot_spk_id=warmup_speaker, stream=True
                ):
                    pass
            except Exception as e:
                print(f"é¢„çƒ­å¤±è´¥: {e}")
                break
    else:
        print("æœªæ‰¾åˆ°å¯ç”¨è¯´è¯äººï¼Œè·³è¿‡é¢„çƒ­")

    print("é¢„çƒ­å®Œæ¯•\n")
    print("=" * 60)
    print(f"ğŸš€ FastCosyVoice3 vLLM TTS Server å¯åŠ¨åœ¨ç«¯å£ {args.port}")
    print(f"   ä½¿ç”¨ vLLM 0.12.0 åŠ é€Ÿ LLM æ¨ç†")
    print(f"   æ”¯æŒ RTX 50 ç³»åˆ— GPU (Blackwell sm_120)")
    print("=" * 60)

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=args.port,
        timeout_keep_alive=60,
        limit_concurrency=100,
        backlog=2048,
    )
